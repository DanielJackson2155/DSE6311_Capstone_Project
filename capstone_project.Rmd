---
title: "DSE6311 Capstone Project"
author: "Daniel Jackson, Nischal Panta, Nelson Tran"
date: "`r Sys.Date()`"
output: word_document
---

There will be a set of initials at the end of each code block representing which contributor wrote that line of code:
DJ for Daniel Jackson
NP for Nischal Panta
NT for Nelson Tran
We will use # all to represent code block we all helped create.

# Loading libraries 
```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(janitor)
library(DMwR2)
library(corrplot)
library(car)
library(psych)
library(pls)
library(caret)
library(MASS)
library(stats)
library(glmnet)

# all
```


# Read in Data
```{r 1}
cov_df <- read.csv("capstone_data.csv")

# Remove X variable, ResponseId variable
cov_df <- cov_df[, -which(names(cov_df) == "X")]
cov_df <- cov_df[, -which(names(cov_df) =="ResponseId")]

# We want to use snakecase on features and simplify features. Before we do that, we need to figure out what features we want to keep and what ones we want to get rid of.
# We already got rid of the X variable from converting our file to a CSV and the ResponseID.

# Data frame dimension
dim(cov_df)
# 2534 observations
# 57 variables

# Print names of variables in data set
colnames(cov_df)

# Check for NA values
colSums(is.na(cov_df))

# DJ
```

# Exploratory Data Analysis
We have multiple variables in our data set that appear more than once. Some of the "_group" variables are other variables in our data set grouped together based on the unique values of other features. For instance, there is a "Educ_Dad" variable and an "Educ_Dad_Group". These "_group" variables have one to three unique values while the the features being grouped have one to seven unique values. To avoid having collinearity with these, we well be removing the "_group" variables.
We also will remove the Classification and Classification_High variable. We will assign the Classification vector to a variable in our global environment just in case we want to compare our results later in our analysis. This variable used z-scores to determine if a student experienced low, medium, or high psychological impact from Covid-19.
Let us also remove the zip code variable. There is 601 NA values and we are not too worried about zip code as we know what college each student is attending.
We can also remove the Source_NCSU variable as our Source variable tells us what school each student attends.
Let us also remove the z-scores of students bad mood and lost time for our analysis.
```{r 3}
cov_df <- cov_df[, -which(names(cov_df) =="Class_Self_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Class_Mom_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Class_Dad_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Income_Relative_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_Mom_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_Dad_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Health_General_group")]
cov_df <- cov_df[, -which(names(cov_df) =="BMI_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Outdoor_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Screen_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Exercise_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Age_18to25")]
cov_df <- cov_df[, -which(names(cov_df) =="Age_3Groups")]
cov_df <- cov_df[, -which(names(cov_df) =="Source_NCSU")]
cov_df <- cov_df[, -which(names(cov_df) =="GIS_ZIP")]
cov_df <- cov_df[, -which(names(cov_df) =="COVID_F1_BadMoods")]
cov_df <- cov_df[, -which(names(cov_df) =="COVID_F2_LostTime")]

# Check for NAs again
colSums(is.na(cov_df))

# 280 NA values in Ethnoracial_Group, NHWhite_f, Ethnoracial_Group_White1_Asian2 
# 8 NA values in Age and Source

# Store Classification into empty vector for now. Might need to revisit this later and re-read in data if remove any observations. And then remove Classification and Classification_High
psycho_class <- c()
pyscho_class <- cov_df$Classification
cov_df <- cov_df[, -which(names(cov_df) =="Classification")]
cov_df <- cov_df[, -which(names(cov_df) =="Classification_High")]

# Check for NAs again
colSums(is.na(cov_df))

# Dimension
dim(cov_df)
# 2534 observations
# 38 variables

# DJ
```


Now let us merge COVID_Afraid, COVID_Irritable, COVID_Guilty, COVID_Sad, COVID_Preoccupied and COVID_Stressed into one response variable called "total_anxiety_level". Before we do that, let us round each observation in each feature to nearest whole number.
```{r 4}
# Which features do we want to round up: COVID_Afraid, COVID_Irritable, COVID_Guilty, COVID_Sad, COVID_Preoccupied and COVID_Stressed
features_round_up <- c("COVID_Afraid", "COVID_Irritable", "COVID_Guilty", "COVID_Sad", "COVID_Preoccupied", "COVID_Stressed")
cov_df <- cov_df %>%
  mutate_at(vars(features_round_up), ~ ceiling(.))

# Now let us merge these into one variable called total_anxiety_level
cov_df <- cov_df %>%
  mutate(total_anxiety_level = COVID_Afraid + COVID_Irritable + 
           COVID_Guilty + COVID_Sad + COVID_Preoccupied)

# Now, let us remove those variables added together from the data set
cov_df <- cov_df %>%
  dplyr::select(-any_of(features_round_up))

# Find average and median value of total_anxiety_level
avg_tot_anx_lev <- mean(cov_df$total_anxiety_level)
# Mean is 251.17
med_tot_anx_lev <- median(cov_df$total_anxiety_level)
# Median is 260

dim(cov_df)
# 2534 observations
# 33 Variables

# DJ
```

Now, let us round any numeric value to then nearest whole number for ease of analysis.
```{r 5}
cov_df <- cov_df %>%
  mutate_if(is.numeric, function(x) round(x))

# We do not have enough information from out data on the Type variable. Let us remove that.
# Also, the binary Educ_College_Grad can be derived from Educ_Self variable. Let us remove that as well.
# Since we already have a Ethnoracial_Group variable, we can remove Ethnoracial_Group_f, NHWhite_f and Ethnoracial_Group_White1_Asian2 
cov_df <- cov_df[, -which(names(cov_df) =="Type")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_College_Grad")]
cov_df <- cov_df[, -which(names(cov_df) =="Ethnoracial_Group_f")]
cov_df <- cov_df[, -which(names(cov_df) =="NHWhite_f")]
cov_df <- cov_df[, -which(names(cov_df) =="Ethnoracial_Group_White1_Asian2")]

dim(cov_df)
# 2534 observations
# 28 variables

# Check for NAs again
colSums(is.na(cov_df))
# Ethnoracial_Group has 280 NA values
# Age has 8

# DJ
```

Next, we need to clean up our features and convert them to snake case for ease of coding/analysis. Then need to decide how we want to handle any remaining NA values.
Please Install the Janitor Package if you haven't already, it allows for snake casing all the variables much easier
```{r 6}
#using the Janitor Package for snake casing each var
cov_df <- cov_df %>%
  clean_names(case = "snake")

# NT
```

We want to see what the dataset looks like, we can plot to see the spread of the data. 
We have character variables that can be turned into factors 
We also have integers which can also be turned into factors
Now the variables have levels.

We also have continuous variables which can be used to create histograms 
The categorical variables can be used to create bar plots
```{r 7}
# Convert character variables and integers to factors w/ levels
cov_df <- cov_df %>% 
  mutate_if(is.character, as.factor)

cov_df <- cov_df %>% 
  mutate_if(is.integer, as.factor)

# Function to create bar plots for categorical variables
plot_cat <- function(df, var) {
  ggplot(df, aes_string(x = var)) +
    geom_bar(fill = sample(colors(), 1)) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.title.y = element_text(size = 12, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    ) +
    labs(title = paste("Count of", var), x = var, y = "Count")
}

# Function to create histograms for continuous variables
plot_cont <- function(df, var) {
  ggplot(df, aes_string(x = var)) +
    geom_histogram(binwidth = 1, fill = sample(colors(), 1), color = "black") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.title.y = element_text(size = 12, face = "bold")
    ) +
    labs(title = paste("Histogram of", var), x = var, y = "Frequency")
}

# Create two separate datasets for plotting
cat_vars <- names(Filter(is.factor, cov_df))
cont_vars <- names(Filter(is.numeric, cov_df))

# Plotting categorical variables
for (cat_vars in cat_vars) {
  print(plot_cat(cov_df, cat_vars))
}

# Plotting all continuous variables
for (cont_vars in cont_vars) {
  print(plot_cont(cov_df, cont_vars))
}

# NP
```

Upon looking at the data, we found that there are a lot of N/A's in "ethnoracial_group" and "age." We thought about either making another category (other) that we could assign to the N/A values or we could use K-nearest-neighbors (KNN) to fill in the missing values.

Just a side note, randomForest could be used to fill in missing quantitative values too, but since the two variables we are using KNN imputation for are qualitative, we do not need to do randomForest.

We wanted to use KNN to fill in the missing values. We will do this after we split our trianing and test data.

Looking at the histograms above, there seems to be a few variables that have outliers, we will use box plots to further visualize them differently. The variables identified are "age," "hrs_outdoor," "hrs_exercise," and "bmi."
```{r 8}
# Making Boxplots for each of the potential variables with outliers.
boxplot(cov_df$age,
        ylab = "age")
boxplot(cov_df$hrs_outdoor,
        ylab = "hrs_outdoor")
boxplot(cov_df$hrs_exercise,
        ylab = "hrs_exercise")
boxplot(cov_df$bmi,
        ylab = "bmi")
# This shows the amount of potential outliers in the boxplots
boxplot.stats(cov_df$age)$out
boxplot.stats(cov_df$hrs_outdoor)$out
boxplot.stats(cov_df$hrs_exercise)$out
boxplot.stats(cov_df$bmi)$out

# NT
```

We have 28 variables in our dataset so far and 25 of them are numerical. There is a chance our variables might be highly correlated to each other which could create issues with our models. To prevent that lets create a correlation matrix and then plot of all our numerical variables to investigate. We can choose a range to be able to see the highly correlated values 
```{r 9}
# Let's select numerical variables again since we made a few edits since our last histogram
numeric_vars <- sapply(cov_df, is.numeric)
num_data <- cov_df[, numeric_vars]

# Creating a simple matrix for all our numerical variable
cor_matrix <- cor(num_data, use = "complete.obs")

print(cor_matrix)
# creating the correlation plot
# lets also make necessary adjustments to make the text smaller 
corrplot::corrplot(cor_matrix, method = "color", type = "full", 
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, 
         title = "Correlation Plot of Numerical Variables", mar = c(0, 0, 1, 0))


# variable "a" will of course be correlated with "a" so lets create a range for significantly correlated variables so absolute value of greater than 0.5 but less than absolute value of 0.99 so we do not get the 1 or -1 values. 
cor_matrix_range <- cor_matrix
cor_matrix_range[abs(cor_matrix_range) < 0.7 | abs(cor_matrix_range) >= 0.99] <- NA


# to get a table of only the values in our range we need to plug it back in 
cor_matrix_range_df <- as.data.frame(as.table(cor_matrix_range))

# lets filter the correlation for our range. Redundancy
fil_cor_matrix_range_df <- cor_matrix_range_df %>%
  filter((Freq > 0.7 & Freq < 0.99) | (Freq < -0.7 & Freq > -0.99))


# now lets print the filtered correlations
print(fil_cor_matrix_range_df)


# lets repeat the plotting process
corrplot::corrplot(cor_matrix_range, method = "color", type = "full", 
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, 
         na.label = " ", 
         title = "Correlation Plot of Numerical Variables (0.7 < |r| < 0.99)", mar = c(0, 0, 1, 0))

# NP
```

Another test for multicollinearity is a VIF Variance Inflation Factor. This is a good measure of the amount of multicollinearity in a set of multiple regression variables. When multicollinearity exists it undermines the statistical significance of an independent variable, increases model complexity and can cause overfitting. It will also inflate the variance (standard error) of coefficient estimates

```{r 10}
# lets fit a linear model with all our numerical variables
multi_col_model <- lm(total_anxiety_level ~ ., data = num_data)

# now lets calculate the Variance Inflation Factor (VIF) to check if we have highly correlated variables. 
vif_values <- vif(multi_col_model)

# As we can see, all the values are fairly low. We are safe
print(vif_values)

#NP
```

Let us take the average of class_dad and class_mom variables and create new variable called class_family. We will round the average up to the nearest whole number and remove the class_dad and class_mom variables after
```{r 11}
cov_df <- cov_df %>%
  mutate(class_family = (class_mom + class_dad)/2)

# Let us round up class_family variable to make it whole and also remove class_dad and class_mom variables
cov_df <- cov_df %>%
  mutate_at(vars(class_family), ~ ceiling(.)) %>%
  dplyr::select(-class_mom) %>%
  dplyr::select(-class_dad)

# DJ
```

We also want to remove the covid_too_much_time predictor.
```{r 12}
cov_df <- cov_df[, -which(names(cov_df) =="covid_too_much_time")]

dim(cov_df)

# Now we are left with 26 predictors.

# DJ
```

Categorical Variables summary that include Frequencies and percentages.
```{r 13}
# Using the tabyl function from Janitor we can create tables that have the frequencies and %'s
cov_df %>%
  tabyl(age) %>%
  adorn_pct_formatting()
cov_df %>%
  tabyl(source) %>%
  adorn_pct_formatting()
cov_df %>%
  tabyl(ethnoracial_group) %>%
  adorn_pct_formatting()

#NT
```
Summary of the continuous variables' mean, median, standard deviation (SD), skewness, and kurtosis
```{r 14}
# We are using our numerical variables that we used for both the histograms, and multicollinearity
cont_vars <- names(Filter(is.numeric, cov_df))

#creating a function that will summarize the numerical variables for the statistics that we want
cont_summary <- function(df, var) {
  cont_summary <- data.frame(
  Mean = mean(df[[var]]),
  Median = median(df[[var]]),
  SD = sd(df[[var]]),
  Skewness = skew(df[[var]]),
  Kurtosis = kurtosi(df[[var]])
  )}

#creating the summary for each numerical var
for (cont_vars in cont_vars) {
  print(cont_summary(cov_df, cont_vars))
}

#NT
```

# Training and Test Data
Let us split our data set into training and test data. We will use R code below to select optimal data split for our training and test data.
```{r 15}
calcSplitRatio <- function(p = NA, df) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  ## If the number of parameters isn't supplied, set it to the number of features minus 1 for the target
  if(is.na(p)) {
    p <- ncol(df) -1   ## COMMENT HERE
  }
  
  ## Calculate the ideal number of testing set
  test_N <- (1/sqrt(p))*nrow(df)
  ## Turn that into a testing proportion
  test_prop <- round((1/sqrt(p))*nrow(df)/nrow(df), 2)
  ## And find the training proportion
  train_prop <- 1-test_prop
  
  ## Tell us the results!
  print(paste0("The ideal split ratio is ", train_prop, ":", test_prop, " training:testing"))
  
  ## Return the size of the training set
  return(train_prop)
}

calcSplitRatio(26, cov_df)
# Ideal training split ratio is 0.8 for training and 0.2 for testing.

# DJ
```

Use value from train_prop for p value in createDataPartition 
```{r 16}
set.seed(123) # set.seed(123) for reproducibility
partition <- createDataPartition(cov_df$total_anxiety_level, p = 0.8, list = FALSE)
train_df <- cov_df[partition,]
test_df <- cov_df[-partition,]

dim(train_df)
# 2029 observations

dim(test_df)
# 505

# DJ
```

# Imputations
Now that the training and test data has been determined, we now will use KNN to fill in the missing values for ethnoracial_group and age predictors.
```{r 17}
#Using K-nearest-neighbors to fill in the missing values
train_df <- knnImputation(train_df, k = 3)
test_df <- knnImputation(test_df, k = 3)

#checking to see if there are any more N/a values
colSums(is.na(train_df))
colSums(is.na(test_df))
unique(train_df$age)
unique(test_df$age)
unique(train_df$ethnoracial_group)
unique(test_df$ethnoracial_group)

# NT
```

# Model Building
In order to start building our first model selection, we want to conduce a Principal Component Analysis first to reduce our dimensions. In order to do so, we need to first encode our three categorical variables: age, source, and ethno_racial group
```{r 18}
length(levels(train_df$age))
length(levels(test_df$age))
# 6 levels

length(levels(train_df$source))
length(levels(test_df$source))
# 7 levels

length(levels(train_df$ethnoracial_group))
length(levels(test_df$ethnoracial_group))
# 4 levels

# DJ
```
We see that age has 6 unique classifiers, source has 7 unique classifiers and ethnoracial_group has 4 levels. We do not want to increase our dimensions of our dataset too much by conducting a one hot or dummy encoding on all three categorical variables. Since ethnoracial_group only has 4, we will conduct a one hot dummy encoding on this variable, which will add 4 more predictors into our data set.

For both age and source, we will conduct a frequency encoding to avoid adding more predictors to our data set. 

Below is the code to encode each of the three categorical variables in both the training and test set.
```{r 19}
# age variable code by NT
#training data
train_df <- train_df %>%
  group_by(age) %>%
  mutate(age_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-age)

#test data
test_df <- test_df %>%
  group_by(age) %>%
  mutate(age_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-age)

# source variable frequency encoding by DJ
# training data
train_df <- train_df %>%
  group_by(source) %>%
  mutate(source_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-source)

# test data
test_df <- test_df %>%
  group_by(source) %>%
  mutate(source_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-source)


# ethnoracial_group variable one-hot encoding
# training data
dummy_vars = dummyVars(~ ethnoracial_group, 
                       data = train_df,
                       levelsOnly = TRUE,
                       fullRank = FALSE)
encoded_ethno_racial_df <- predict(dummy_vars, newdata = train_df)
# convert col names to lower case
encoded_ethno_racial_df <- encoded_ethno_racial_df %>%
  clean_names(case = "snake")
train_df <- cbind(train_df, encoded_ethno_racial_df)
# Remove ethno_racial variable
train_df <- train_df %>%
  dplyr::select(-ethnoracial_group)

# test data
dummy_vars = dummyVars(~ ethnoracial_group, 
                       data = test_df,
                       levelsOnly = TRUE,
                       fullRank = FALSE)
encoded_ethno_racial_df <- predict(dummy_vars, newdata = test_df)
# convert col names to lower case
encoded_ethno_racial_df <- encoded_ethno_racial_df %>%
  clean_names(case = "snake")
test_df <- cbind(test_df, encoded_ethno_racial_df)
# Remove ethno_racial variable
test_df <- test_df %>%
  dplyr::select(-ethnoracial_group)

# Now, for visual purposes in our data sets, let us move our response variable to the far right of all of our predictors.
train_df <- train_df %>%
  dplyr::select(-total_anxiety_level, total_anxiety_level)
# row names changed from 1 to x1 and so on. Let's reset that.
rownames(train_df) <- NULL

test_df <- test_df %>%
  dplyr::select(-total_anxiety_level, total_anxiety_level)
# row names changed from 1 to x1 and so on. Let's reset that.
rownames(test_df) <- NULL


# DJ/NT
```

## Response Variable Transformation
Before we center and scale our predictors to prepare for our PCA. We want to revisit the distribution of our response variable total_anxiety_level. We will look at both a histogram and Q-Q plot to see if we need to transform our response variable or not. We will also conduct the Shapiro-Wilk Test to see if our response variable has a normal distribution or not. If the p-value, of the test is less than 0.05, then we reject the null hypothesis that the distribution is normal.
```{r 20}
# histogram
hist(cov_df$total_anxiety_level,
     xlab = "Total Anxiety Level",
     main = "Total Anxiety Level Distribution")

# q-q plot
qqnorm(cov_df$total_anxiety_level, main = "Q-Q Plot of Total Anxiety Level")
qqline(cov_df$total_anxiety_level, col = "red")


# Shapiro-Wilk Test
print(shapiro.test(cov_df$total_anxiety_level))

# DJ
```
The histogram looks like Total Anxiety Level relatively follows a normal distribution. However, when we look at the Q-Q plot, we see that the plot does not fully follow the reference line from -2 to 2 on the Theoretical Quantiles axis. This is proven by Shapiro-Wilk Test. With a p-value of 3.072e-14, we reject the null hypothesis and state that the distribution is not normal. Therefore, we need to transform our response variable. Below, we will try a few transformationts to see if that makes the distribution more normal.

Square Root Transformation:
```{r 21}
# Square Root
# histogram
hist(sqrt(cov_df$total_anxiety_level),
     xlab = "Square Root(Total Anxiety Level)",
     main = "Square Root of Total Anxiety Level Distribution")

# q-q plot
qqnorm(sqrt(cov_df$total_anxiety_level), main = "Q-Q Plot of sqrt(Total Anxiety Level)")
qqline(sqrt(cov_df$total_anxiety_level), col = "red")


# Shapiro-Wilk Test
print(shapiro.test(sqrt(cov_df$total_anxiety_level)))

# DJ
```
Based on the histogram, Q-Q plot, and the Shapiro-Wilk Test, the square root transformation did not make the distribution more normal.

Squared:
```{r 22}
# Squared
# histogram
hist((cov_df$total_anxiety_level)^2,
     xlab = "(Total Anxiety Level)^2",
     main = "Total Anxiety Level Squared Distribution")

# q-q plot
qqnorm((cov_df$total_anxiety_level)^2, main = "Q-Q Plot of (Total Anxiety Level)^2")
qqline((cov_df$total_anxiety_level)^2, col = "red")


# Shapiro-Wilk Test
print(shapiro.test((cov_df$total_anxiety_level)^2))

# DJ
```
Based on the histogram, Q-Q plot, and the Shapiro-Wilk Test, the squared transformation did not make the distribution more normal.

Log Transformation:
```{r 23}
# Log
# histogram
hist(log(cov_df$total_anxiety_level),
     xlab = "log(Total Anxiety Level)",
     main = "Log Total Anxiety Level Distribution")

# DJ
```

Just looking at the log transformation histogram, we see that the transformation did not make the response variable more normal.

Let us try the Box-Cox transformation:
```{r 24}
# Box-Cox transformation from MASS package
# First, we need to ensure our response variable is positive before transforming.
range(cov_df$total_anxiety_level)
# 0 to 500 is our range.
# Therefore, we need to shift data to ensure it is all positive. We will shift by 1.
cov_df$total_anxiety_level_shift <- cov_df$total_anxiety_level - min(cov_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = cov_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# Optimal lambda is 1.1

# Box-Cox transformation function
boxcox_transform <- function(x, lambda) {
  if (lambda == 0) {
    return(log(x))  # special case for when lambda = 0
  } else {
    return((x^lambda - 1) / lambda)
  }
}

# Now we apply transformation to total_anxiety_level_shift
cov_df$box_cox_anxiety_level <- boxcox_transform(cov_df$total_anxiety_level_shift, lambda_opt)

# histogram
hist(cov_df$box_cox_anxiety_level,
     xlab = "Box-Cox Total Anxiety Level",
     main = "Box-Cox Total Anxiety Level Distribution")

qqnorm(cov_df$box_cox_anxiety_level, main = "Q-Q Plot of Box-Cox Total Anxiety Level")
qqline(cov_df$box_cox_anxiety_level, col = "red")


# Shapiro-Wilk Test
print(shapiro.test(cov_df$box_cox_anxiety_level))

# DJ
```
After our Box-Cox transformation, we see that our p-value of the Shapiro-Wilk Test is still not greater than 0.05. However, after our Box-Cox transformation, the p-value is much closer to 0.05 than our initial p-value. Therefore, since the p-value is closer to 0.05, we should use a Box-Cox transformation on our response variable for both our training and our test data sets.
```{r 25}
orig_shapiro = shapiro.test(cov_df$total_anxiety_level)
bc_shapiro = shapiro.test(cov_df$box_cox_anxiety_level)

orig_p_val = orig_shapiro$p.value
bc_p_val = bc_shapiro$p.value

print(paste("Original p-value:", orig_p_val))
print(paste("Box-Cox transformed p-value:", bc_p_val))

if (bc_p_val > orig_p_val) {
  print("Yes, p-value after Box-Cox transformation is greater than original p-value")
} else {
  print("No, p-value after Box-Cox transformation is not greater than original p-value")
}

# Now, let us Box-Cox transform our response variable in our training and test data
# Training
range(train_df$total_anxiety_level)
# 0 to 500
train_df$total_anxiety_level_shift <- train_df$total_anxiety_level - min(train_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = train_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# 1.1 again
train_df$box_cox_anxiety_level <- boxcox_transform(train_df$total_anxiety_level_shift, lambda_opt)

# Now, let us remove total_anxiety_level and total_anxiety_level_shift from train_df then rename the box_cox_anxiety_level to total_anxiety_level. Let us also round to the nearest whole number
train_df <- train_df %>%
  dplyr::select(-total_anxiety_level, -total_anxiety_level_shift) %>%
  dplyr::rename(total_anxiety_level = box_cox_anxiety_level) %>%
  dplyr::mutate(total_anxiety_level = round(total_anxiety_level))

# Test Data
range(test_df$total_anxiety_level)
# 0 to 500
test_df$total_anxiety_level_shift <- test_df$total_anxiety_level - min(test_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = test_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# 1.1 again
test_df$box_cox_anxiety_level <- boxcox_transform(test_df$total_anxiety_level_shift, lambda_opt)

# Now, let us remove total_anxiety_level and total_anxiety_level_shift from train_df then rename the box_cox_anxiety_level to total_anxiety_level. Let us also round to the nearest whole number
test_df <- test_df %>%
  dplyr::select(-total_anxiety_level, -total_anxiety_level_shift) %>%
  dplyr::rename(total_anxiety_level = box_cox_anxiety_level) %>%
  dplyr::mutate(total_anxiety_level = round(total_anxiety_level))

# DJ
```

Now that we have properly transformed our response variable, we can now center and scale our predictors to prepare for PCA.

# Center and Scale Predictors
```{r 26}
#training data

#storing the response variable into a vector (total_anxiety_level)
anx_lev_train <- train_df$total_anxiety_level

#creating a scaled_train_df without response variable
scaled_train_df <- train_df[,-29]

#centering and scaling using standard deviation of 2
scaled_train_df <- scale(scaled_train_df, center = TRUE, scale = TRUE)
apply(scaled_train_df, 2, sd)

#adding response variable back in scaled df
scaled_train_df <- cbind(scaled_train_df, anx_lev_train)

#test data

#storing the response variable into a vector (total_anxiety_level)
anx_lev_test <- test_df$total_anxiety_level

#creating a scaled_test_df without response variable
scaled_test_df <- test_df[,-29]

#centering and scaling using standard deviation of 2
scaled_test_df <- scale(scaled_test_df, center = TRUE, scale = TRUE)
apply(scaled_test_df, 2, sd)

#adding response variable back in scaled df
scaled_test_df <- cbind(scaled_test_df, anx_lev_test)

# NT
```
# PCA (principal component analysis) data reduction
```{r 27}
# Need to convert matrix to data frame for training and test data
scaled_train_df = as.data.frame(scaled_train_df)
scaled_test_df = as.data.frame(scaled_test_df)

# Separate predictors and response
train_pred <- scaled_train_df %>% 
  dplyr::select(-anx_lev_train)  # Exclude response variable
train_resp <- scaled_train_df$anx_lev_train  # Extract response variable

# Perform PCA
pca_train <- prcomp(train_pred, center = FALSE, scale. = FALSE)
# predictors already trained and scaled

# View PCA summary
summary(pca_train)

# Look at scree plot
eigenvalues <- pca_train$sdev^2

# Compute the proportion of variance explained by each component
variance_explained <- eigenvalues / sum(eigenvalues)

# Plot manually
plot(variance_explained, type = "b", main = "Scree Plot", xlab = "Principal Component", ylab = "Proportion of Variance Explained")

# DJ
```

Looks like we have a few elbow that we can point to in our scree plot. We see the proportion of variance explained tend to level out after the 5th, 10th, 15th and 19th principal components. The drop off in proportion of variance explained is relatively small when comparing those principal components. However, the proportion of variance explained really seems to level off after the 19th principal component. In that case, let us change the scaled trained data set to a new data set with those 19 principal components and add the response variable as well. Let us convert that new data set to a data frame.

```{r 30}
pca_scores <- pca_train$x[, 1:19]
# Combine the reduced principal components with the response variable
reduced_train_df <- cbind(pca_scores, train_resp)
# Convert to data frame
reduced_train_df = as.data.frame(reduced_train_df)

# DJ
```

Now let us do the same for the test model.
```{r 31}
# Separate predictors and response
test_pred <- scaled_test_df %>% 
  dplyr::select(-anx_lev_test)  # Exclude response variable
test_resp <- scaled_test_df$anx_lev_test  # Extract response variable

# Perform PCA
pca_test <- prcomp(test_pred, center = FALSE, scale. = FALSE)
# predictors already trained and scaled

# View PCA summary
summary(pca_test)

# Look at scree plot
eigenvalues <- pca_test$sdev^2

# Compute the proportion of variance explained by each component
variance_explained <- eigenvalues / sum(eigenvalues)

# Plot manually
plot(variance_explained, type = "b", main = "Scree Plot", xlab = "Principal Component", ylab = "Proportion of Variance Explained")

pca_scores_test <- pca_test$x[, 1:19]
# Combine the reduced principal components with the response variable
reduced_test_df <- cbind(pca_scores_test, test_resp)
# Convert to data frame
reduced_test_df = as.data.frame(reduced_test_df)

# DJ
```

Now we can start fitting our first model!

# Intial model building 
## Multiple Linear Regression Model
Let us first just try a simple linear regression model using backwards selection. We will start by fitting a model on the reduced training data frame. For any predictor that has a p-value greater than 0.05, we will remove that predictor
```{r 32}
cov_lm = lm(train_resp ~., reduced_train_df)
summary(cov_lm)

# DJ
```

Let us remove PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC18, and PC19.
```{r 33}
feats_to_remove = c("PC10", "PC11", "PC12", "PC13", "PC14", "PC15", "PC16", "PC18", "PC19")
reduced_train_df_1 = reduced_train_df %>%
  dplyr::select(-feats_to_remove)
cov_lm = lm(train_resp ~., reduced_train_df_1)
summary(cov_lm)

# DJ
```
Now, all of the p-values for each principal component are less than 0.05. Let us test the mean squared error rate on the training data itself.
```{r 34}
pred_train = predict(cov_lm, newdata = reduced_train_df_1)
mean((pred_train - reduced_train_df_1$train_resp)^2)

# DJ
```

Now try on test data
```{r 35}
pred_test = predict(cov_lm, newdata = reduced_test_df)
mean((pred_test - reduced_test_df$test_resp)^2)

# DJ
```

We see a very high mean squared error rate for both the training and test mean squared error rates. 

## Lasso Regression
```{r 36}
library(glmnet)
# Perform lasso regression
set.seed(123)
train_matrix = model.matrix(train_resp~., reduced_train_df)
test_matrix = model.matrix(test_resp~., reduced_test_df)
# Select lambda using cross-validation
cv_lam = cv.glmnet(train_matrix, reduced_train_df$train_resp, alpha = 1)
best_lam = cv_lam$lambda.min
best_lam
# Lambda chosen by cross-validation is 1.074
# Now we fit lasso regression model and predict train data
cov_lasso = glmnet(train_matrix, reduced_train_df$train_resp, alpha = 1)
pred_lasso = predict(cov_lasso, s = best_lam, newx = train_matrix)
# Find training error:
mean((pred_lasso - reduced_train_df$train_resp)^2)

# Now we fit lasso regression model and predict test data
pred_lasso = predict(cov_lasso, s = best_lam, newx = test_matrix)
# Find training error:
mean((pred_lasso - reduced_test_df$test_resp)^2)

# DJ
```


## Ordinal Logistic Regression
For our first model, we are going to build an ordinal logistic regression model. For that we need to first turn our response variable into ordinal. 

Firsts lets see the spread of of our variable. 
```{r 36}
summary(reduced_train_df$train_resp)

#NP
```

```{r 37}
# Its better to visualize using a histogram. Lets plot a basic one. 

hist(reduced_train_df$train_resp,
     main = "Histogram of Total Anxiety Level",
     xlab = "Total Anxiety Level",
     ylab = "Frequency",
     col = "skyblue",
     border = "black",
     breaks = 30)  
#NP
```
To turn total_anixety_level we need to first break it into intervals and then create the ordinal variable. 

```{r 38}
# the breaks will be from 0-100, 101-199, 200-299 and so forth,

breaks <- c(-Inf, 100, 199, 299, 399, 499, 599, 699, 799, Inf)
labels <- 1:9

# And now lets put them into the variable 
reduced_train_df$or_train_resp<- cut(reduced_train_df$train_resp,
                                        breaks = breaks,
                                        labels = labels,
                                        right = TRUE)

#this code shows weather or not it works
head(reduced_train_df$or_train_resp)
#NP
```

```{r 39}
# now lets do the same with our test variable
breaks <- c(-Inf, 100, 199, 299, 399, 499, 599, 699, 799, Inf)
labels <- 1:9

reduced_test_df$or_test_resp <- cut(reduced_test_df$test_resp,
                                        breaks = breaks,
                                        labels = labels,
                                        right = TRUE)
head(reduced_test_df$or_test_resp)
#NP
```

```{r 40}
#this code is redundant but it ensures that both of the data sets variables are ordinal 

reduced_train_df$or_train_resp <- factor(reduced_train_df$or_train_resp,
                                          levels = 1:9,
                                          ordered = TRUE)


reduced_test_df$or_test_resp <- factor(reduced_test_df$or_test_response,
                                          levels = 1:9,
                                          ordered = TRUE)
#NP
```

```{r 41}
#now lets see the plot for the distribution of the variable 

ggplot(reduced_train_df, aes(x = or_train_resp)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Ordinal Anxiety Levels",
       x = "Ordinal Anxiety Level",
       y = "Count") +
  theme_minimal()
#NP
```


```{r 42}
#and now some test stats
table(reduced_train_df$or_train_resp)
table(reduced_test_df$or_test_resp)

summary(reduced_train_df$or_train_resp)
summary(reduced_test_df$or_test_resp)

#NP
```

Now let us fit an ordinal logistic regression model
```{r 43}
cov_or <- polr(or_train_resp ~. -train_resp, data = reduced_train_df, Hess = TRUE)
summary(cov_or)

# NP
```

# Throwaway Code
Typically anything that is above the ((75th quartile) + 1.5 * interquartile range (IQR)) or below ((75th quartile) - 1.5 * interquartile range (IQR)) are potential outliers and are identified by the boxplot.stats function. The variables "age," "hrs_outdoor," "hrs_exercise," are qualitative variables so we won't strictly adhere to these outliers identified by the boxplox.stats function. As for "bmi," this is a quantitative continuous variable and will use this function identify the potential outliers.
```{r throw 1}
#this will remove replace all of the potential outliers with N/a values
for (x in c("bmi"))
{
  value = cov_df[,x][cov_df[,x] %in% boxplot.stats(cov_df[,x])$out]
  cov_df[,x][cov_df[,x] %in% value] = NA
}
#This will remove all of the missing values from the columns
cov_df <- drop_na(cov_df)
#checking to see if the N/a values are removed from the data set
colSums(is.na(cov_df))

#NT
```

I thought there there were a few observations from the qualitative variables that I would consider as outliers and wanted to get rid of them in the case that they will affect our models in a bad way.
```{r throw 2}
#creating a cut off for hours of exercise and then subsetting those that aren't wanted from the data frame
threshold_hrs_exercise <- 9 
cov_df <- subset(cov_df, hrs_exercise < threshold_hrs_exercise)
#creating an array of which ages that should be kept in the data frame
ages_to_keep <- c("18 to 24", "25 to 32", "33 to 44", "45 to 54", "55 to 64")
cov_df <- filter(cov_df, age %in% ages_to_keep)

#NT
```

```{r throw 3}
#performing 
pca_test <- prcomp(scaled_test_df)
str(pca_test)

library(factoextra)
fviz_eig(pca_test, addlabels = TRUE, ylim = c(0,100))

#NT
```

## Regression Tree
```{r}
library(tree)
cbm_tree = tree(train_resp ~ ., reduced_train_df)
summary(cbm_tree)
plot(cbm_tree)
text(cbm_tree, pretty = 0)

yhat = predict(cbm_tree, newdata = reduced_train_df)
mean((yhat - reduced_train_df$train_resp)^2)

yhat = predict(cbm_tree, newdata = reduced_test_df)
mean((yhat - reduced_test_df$test_resp)^2)

# DJ
```

## Lasso Regression
```{r 36}
library(glmnet)
# Perform lasso regression
set.seed(123)
train_matrix = model.matrix(train_resp~., reduced_train_df)
test_matrix = model.matrix(test_resp~., reduced_test_df)
# Select lambda using cross-validation
cv_lam = cv.glmnet(train_matrix, reduced_train_df$train_resp, alpha = 1)
best_lam = cv_lam$lambda.min
best_lam
# Lambda chosen by cross-validation is 1.074
# Now we fit lasso regression model and predict train data
cov_lasso = glmnet(train_matrix, reduced_train_df$train_resp, alpha = 1)
pred_lasso = predict(cov_lasso, s = best_lam, newx = train_matrix)
# Find training error:
mean((pred_lasso - reduced_train_df$train_resp)^2)

# Now we fit lasso regression model and predict test data
pred_lasso = predict(cov_lasso, s = best_lam, newx = test_matrix)
# Find training error:
mean((pred_lasso - reduced_test_df$test_resp)^2)

# DJ
```