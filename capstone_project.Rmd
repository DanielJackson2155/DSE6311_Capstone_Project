---
title: "DSE6311 Capstone Project"
author: "Daniel Jackson, Nischal Panta, Nelson Tran"
date: "`r Sys.Date()`"
output: word_document
---

There will be a set of initials at the end of each code block representing which contributor wrote that line of code:
DJ for Daniel Jackson
NP for Nischal Panta
NT for Nelson Tran
We will use # all to represent code block we all helped create.

# Loading libraries 
```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(janitor)
library(DMwR2)
library(corrplot)
library(car)
library(psych)
library(pls)
library(caret)
library(MASS)
library(stats)
library(glmnet)
library(keras)
library(tensorflow)
library(pROC)
library(tree)
library(e1071)
library(mlr)
# all
```

# Read in Data
```{r 1}
cov_df <- read.csv("capstone_data.csv")

# Remove X variable, ResponseId variable
cov_df <- cov_df[, -which(names(cov_df) == "X")]
cov_df <- cov_df[, -which(names(cov_df) =="ResponseId")]

# We want to use snakecase on features and simplify features. Before we do that, we need to figure out what features we want to keep and what ones we want to get rid of.
# We already got rid of the X variable from converting our file to a CSV and the ResponseID.

# Data frame dimension
dim(cov_df)
# 2534 observations
# 57 variables

# Print names of variables in data set
colnames(cov_df)

# Check for NA values
colSums(is.na(cov_df))

# DJ
```

# Exploratory Data Analysis
We have multiple variables in our data set that appear more than once. Some of the "_group" variables are other variables in our data set grouped together based on the unique values of other features. For instance, there is a "Educ_Dad" variable and an "Educ_Dad_Group". These "_group" variables have one to three unique values while the the features being grouped have one to seven unique values. To avoid having collinearity with these, we well be removing the "_group" variables.
We also will remove the Classification and Classification_High variable. We will assign the Classification vector to a variable in our global environment just in case we want to compare our results later in our analysis. This variable used z-scores to determine if a student experienced low, medium, or high psychological impact from Covid-19.
Let us also remove the zip code variable. There is 601 NA values and we are not too worried about zip code as we know what college each student is attending.
We can also remove the Source_NCSU variable as our Source variable tells us what school each student attends.
Let us also remove the z-scores of students bad mood and lost time for our analysis.
```{r 2}
cov_df <- cov_df[, -which(names(cov_df) =="Class_Self_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Class_Mom_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Class_Dad_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Income_Relative_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_Mom_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_Dad_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Health_General_group")]
cov_df <- cov_df[, -which(names(cov_df) =="BMI_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Outdoor_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Screen_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Exercise_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Age_18to25")]
cov_df <- cov_df[, -which(names(cov_df) =="Age_3Groups")]
cov_df <- cov_df[, -which(names(cov_df) =="Source_NCSU")]
cov_df <- cov_df[, -which(names(cov_df) =="GIS_ZIP")]
cov_df <- cov_df[, -which(names(cov_df) =="COVID_F1_BadMoods")]
cov_df <- cov_df[, -which(names(cov_df) =="COVID_F2_LostTime")]

# Check for NAs again
colSums(is.na(cov_df))

# 280 NA values in Ethnoracial_Group, NHWhite_f, Ethnoracial_Group_White1_Asian2 
# 8 NA values in Age and Source

# Store Classification into empty vector for now. Might need to revisit this later and re-read in data if remove any observations. And then remove Classification and Classification_High
psycho_class <- c()
pyscho_class <- cov_df$Classification
cov_df <- cov_df[, -which(names(cov_df) =="Classification")]
cov_df <- cov_df[, -which(names(cov_df) =="Classification_High")]

# Check for NAs again
colSums(is.na(cov_df))

# Dimension
dim(cov_df)
# 2534 observations
# 38 variables

# DJ
```


Now let us merge COVID_Afraid, COVID_Irritable, COVID_Guilty, COVID_Sad, COVID_Preoccupied and COVID_Stressed into one response variable called "total_anxiety_level". Before we do that, let us round each observation in each feature to nearest whole number.
```{r 3}
# Which features do we want to round up: COVID_Afraid, COVID_Irritable, COVID_Guilty, COVID_Sad, COVID_Preoccupied and COVID_Stressed
features_round_up <- c("COVID_Afraid", "COVID_Irritable", "COVID_Guilty", "COVID_Sad", "COVID_Preoccupied", "COVID_Stressed")
cov_df <- cov_df %>%
  mutate_at(vars(features_round_up), ~ ceiling(.))

# Now let us merge these into one variable called total_anxiety_level
cov_df <- cov_df %>%
  mutate(total_anxiety_level = COVID_Afraid + COVID_Irritable + 
           COVID_Guilty + COVID_Sad + COVID_Preoccupied)

# Now, let us remove those variables added together from the data set
cov_df <- cov_df %>%
  dplyr::select(-any_of(features_round_up))

# Let us take average of total anxiety level
cov_df <- cov_df %>%
  mutate(total_anxiety_level = (total_anxiety_level)/6)
# Round up each response variable
cov_df <- cov_df %>%
  mutate_at(vars(total_anxiety_level), ~ ceiling(.))

# Find average and median value of avg_anxiety_level
avg_tot_anx_lev <- mean(cov_df$total_anxiety_level)
# Mean is 42.27
med_tot_anx_lev <- median(cov_df$total_anxiety_level)
# Median is 44

dim(cov_df)
# 2534 observations
# 33 Variables

# DJ
```

Now, let us round any numeric value to then nearest whole number for ease of analysis.
```{r 4}
cov_df <- cov_df %>%
  mutate_if(is.numeric, function(x) round(x))

# We do not have enough information from out data on the Type variable. Let us remove that.
# Also, the binary Educ_College_Grad can be derived from Educ_Self variable. Let us remove that as well.
# Since we already have a Ethnoracial_Group variable, we can remove Ethnoracial_Group_f, NHWhite_f and Ethnoracial_Group_White1_Asian2 
cov_df <- cov_df[, -which(names(cov_df) =="Type")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_College_Grad")]
cov_df <- cov_df[, -which(names(cov_df) =="Ethnoracial_Group_f")]
cov_df <- cov_df[, -which(names(cov_df) =="NHWhite_f")]
cov_df <- cov_df[, -which(names(cov_df) =="Ethnoracial_Group_White1_Asian2")]

dim(cov_df)
# 2534 observations
# 28 variables

# Check for NAs again
colSums(is.na(cov_df))
# Ethnoracial_Group has 280 NA values
# Age has 8

# DJ
```

Next, we need to clean up our features and convert them to snake case for ease of coding/analysis. Then need to decide how we want to handle any remaining NA values.
Please Install the Janitor Package if you haven't already, it allows for snake casing all the variables much easier
```{r 5}
#using the Janitor Package for snake casing each var
cov_df <- cov_df %>%
  clean_names(case = "snake")

# NT
```

We want to see what the dataset looks like, we can plot to see the spread of the data. 
We have character variables that can be turned into factors 
We also have integers which can also be turned into factors
Now the variables have levels.

We also have continuous variables which can be used to create histograms 
The categorical variables can be used to create bar plots
```{r 6}
# Convert character variables and integers to factors w/ levels
cov_df <- cov_df %>% 
  mutate_if(is.character, as.factor)

cov_df <- cov_df %>% 
  mutate_if(is.integer, as.factor)

# Function to create bar plots for categorical variables
plot_cat <- function(df, var) {
  ggplot(df, aes_string(x = var)) +
    geom_bar(fill = sample(colors(), 1)) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.title.y = element_text(size = 12, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1)
    ) +
    labs(title = paste("Count of", var), x = var, y = "Count")
}

# Function to create histograms for continuous variables
plot_cont <- function(df, var) {
  ggplot(df, aes_string(x = var)) +
    geom_histogram(binwidth = 1, fill = sample(colors(), 1), color = "black") +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      axis.title.x = element_text(size = 12, face = "bold"),
      axis.title.y = element_text(size = 12, face = "bold")
    ) +
    labs(title = paste("Histogram of", var), x = var, y = "Frequency")
}

# Create two separate datasets for plotting
cat_vars <- names(Filter(is.factor, cov_df))
cont_vars <- names(Filter(is.numeric, cov_df))

# Plotting categorical variables
for (cat_vars in cat_vars) {
  print(plot_cat(cov_df, cat_vars))
}

# Plotting all continuous variables
for (cont_vars in cont_vars) {
  print(plot_cont(cov_df, cont_vars))
}

# NP
```

Upon looking at the data, we found that there are a lot of N/A's in "ethnoracial_group" and "age." We thought about either making another category (other) that we could assign to the N/A values or we could use K-nearest-neighbors (KNN) to fill in the missing values.

Just a side note, randomForest could be used to fill in missing quantitative values too, but since the two variables we are using KNN imputation for are qualitative, we do not need to do randomForest.

We wanted to use KNN to fill in the missing values. We will do this after we split our training and test data.

Looking at the histograms above, there seems to be a few variables that have outliers, we will use box plots to further visualize them differently. The variables identified are "age," "hrs_outdoor," "hrs_exercise," and "bmi."
```{r 7}
# Making Boxplots for each of the potential variables with outliers.
boxplot(cov_df$age,
        ylab = "age")
boxplot(cov_df$hrs_outdoor,
        ylab = "hrs_outdoor")
boxplot(cov_df$hrs_exercise,
        ylab = "hrs_exercise")
boxplot(cov_df$bmi,
        ylab = "bmi")
# This shows the amount of potential outliers in the boxplots
boxplot.stats(cov_df$age)$out
boxplot.stats(cov_df$hrs_outdoor)$out
boxplot.stats(cov_df$hrs_exercise)$out
boxplot.stats(cov_df$bmi)$out

# NT
```

We have 28 variables in our dataset so far and 25 of them are numerical. There is a chance our variables might be highly correlated to each other which could create issues with our models. To prevent that lets create a correlation matrix and then plot of all our numerical variables to investigate. We can choose a range to be able to see the highly correlated values 
```{r 8}
# Let's select numerical variables again since we made a few edits since our last histogram
numeric_vars <- sapply(cov_df, is.numeric)
num_data <- cov_df[, numeric_vars]

# Creating a simple matrix for all our numerical variable
cor_matrix <- cor(num_data, use = "complete.obs")

print(cor_matrix)
# creating the correlation plot
# lets also make necessary adjustments to make the text smaller 
corrplot::corrplot(cor_matrix, method = "color", type = "full", 
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, 
         title = "Correlation Plot of Numerical Variables", mar = c(0, 0, 1, 0))


# variable "a" will of course be correlated with "a" so lets create a range for significantly correlated variables so absolute value of greater than 0.5 but less than absolute value of 0.99 so we do not get the 1 or -1 values. 
cor_matrix_range <- cor_matrix
cor_matrix_range[abs(cor_matrix_range) < 0.7 | abs(cor_matrix_range) >= 0.99] <- NA


# to get a table of only the values in our range we need to plug it back in 
cor_matrix_range_df <- as.data.frame(as.table(cor_matrix_range))

# lets filter the correlation for our range. Redundancy
fil_cor_matrix_range_df <- cor_matrix_range_df %>%
  filter((Freq > 0.7 & Freq < 0.99) | (Freq < -0.7 & Freq > -0.99))


# now lets print the filtered correlations
print(fil_cor_matrix_range_df)


# lets repeat the plotting process
corrplot::corrplot(cor_matrix_range, method = "color", type = "full", 
         tl.col = "black", tl.srt = 45, tl.cex = 0.8, 
         na.label = " ", 
         title = "Correlation Plot of Numerical Variables (0.7 < |r| < 0.99)", mar = c(0, 0, 1, 0))

# NP
```

Another test for multicollinearity is a VIF Variance Inflation Factor. This is a good measure of the amount of multicollinearity in a set of multiple regression variables. When multicollinearity exists it undermines the statistical significance of an independent variable, increases model complexity and can cause overfitting. It will also inflate the variance (standard error) of coefficient estimates

```{r 9}
# lets fit a linear model with all our numerical variables
multi_col_model <- lm(total_anxiety_level ~ ., data = num_data)

# now lets calculate the Variance Inflation Factor (VIF) to check if we have highly correlated variables. 
vif_values <- vif(multi_col_model)

# As we can see, all the values are fairly low. We are safe
print(vif_values)

#NP
```

Let us take the average of class_dad and class_mom variables and create new variable called class_family. We will round the average up to the nearest whole number and remove the class_dad and class_mom variables after
```{r 10}
cov_df <- cov_df %>%
  mutate(class_family = (class_mom + class_dad)/2)

# Let us round up class_family variable to make it whole and also remove class_dad and class_mom variables
cov_df <- cov_df %>%
  mutate_at(vars(class_family), ~ ceiling(.)) %>%
  dplyr::select(-class_mom) %>%
  dplyr::select(-class_dad)

# DJ
```

We also want to remove the covid_too_much_time predictor.
```{r 11}
cov_df <- cov_df[, -which(names(cov_df) =="covid_too_much_time")]

dim(cov_df)

# Now we are left with 26 predictors.

# DJ
```

Categorical Variables summary that include Frequencies and percentages.
```{r 12}
# Using the tabyl function from Janitor we can create tables that have the frequencies and %'s
cov_df %>%
  tabyl(age) %>%
  adorn_pct_formatting()
cov_df %>%
  tabyl(source) %>%
  adorn_pct_formatting()
cov_df %>%
  tabyl(ethnoracial_group) %>%
  adorn_pct_formatting()

#NT
```
Summary of the continuous variables' mean, median, standard deviation (SD), skewness, and kurtosis
```{r 13}
# We are using our numerical variables that we used for both the histograms, and multicollinearity
cont_vars <- names(Filter(is.numeric, cov_df))

#creating a function that will summarize the numerical variables for the statistics that we want
cont_summary <- function(df, var) {
  cont_summary <- data.frame(
  Mean = mean(df[[var]]),
  Median = median(df[[var]]),
  SD = sd(df[[var]]),
  Skewness = skew(df[[var]]),
  Kurtosis = kurtosi(df[[var]])
  )}

#creating the summary for each numerical var
for (cont_vars in cont_vars) {
  print(cont_summary(cov_df, cont_vars))
}

#NT
```

# Training and Test Data
Let us split our data set into training and test data. We will use R code below to select optimal data split for our training and test data.
```{r 14}
calcSplitRatio <- function(p = NA, df) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  ## If the number of parameters isn't supplied, set it to the number of features minus 1 for the target
  if(is.na(p)) {
    p <- ncol(df) -1   ## COMMENT HERE
  }
  
  ## Calculate the ideal number of testing set
  test_N <- (1/sqrt(p))*nrow(df)
  ## Turn that into a testing proportion
  test_prop <- round((1/sqrt(p))*nrow(df)/nrow(df), 2)
  ## And find the training proportion
  train_prop <- 1-test_prop
  
  ## Tell us the results!
  print(paste0("The ideal split ratio is ", train_prop, ":", test_prop, " training:testing"))
  
  ## Return the size of the training set
  return(train_prop)
}

calcSplitRatio(26, cov_df)
# Ideal training split ratio is 0.8 for training and 0.2 for testing.

# DJ
```

Use value from train_prop for p value in createDataPartition 
```{r 15}
set.seed(123) # set.seed(123) for reproducibility
partition <- createDataPartition(cov_df$total_anxiety_level, p = 0.8, list = FALSE)
train_df <- cov_df[partition,]
test_df <- cov_df[-partition,]

dim(train_df)
# 2029 observations

dim(test_df)
# 505

# DJ
```

# Imputations
Now that the training and test data has been determined, we now will use KNN to fill in the missing values for ethnoracial_group and age predictors.
```{r 16}
#Using K-nearest-neighbors to fill in the missing values
train_df <- knnImputation(train_df, k = 3)
test_df <- knnImputation(test_df, k = 3)

#checking to see if there are any more N/a values
colSums(is.na(train_df))
colSums(is.na(test_df))
unique(train_df$age)
unique(test_df$age)
unique(train_df$ethnoracial_group)
unique(test_df$ethnoracial_group)

# NT
```

# Model Building
## Response Variable Transformation
Before we center and scale our predictors to prepare for our PCA. We want to revisit the distribution of our response variable total_anxiety_level. We will look at both a histogram and Q-Q plot to see if we need to transform our response variable or not. We will also conduct the Shapiro-Wilk Test to see if our response variable has a normal distribution or not. If the p-value, of the test is less than 0.05, then we reject the null hypothesis that the distribution is normal.
```{r 17}
# histogram
hist(cov_df$total_anxiety_level,
     xlab = "Total Anxiety Level",
     main = "Total Anxiety Level Distribution")

# q-q plot
qqnorm(cov_df$total_anxiety_level, main = "Q-Q Plot of Total Anxiety Level")
qqline(cov_df$total_anxiety_level, col = "red")


# Shapiro-Wilk Test
print(shapiro.test(cov_df$total_anxiety_level))

# DJ
```
The histogram looks like Total Anxiety Level relatively follows a normal distribution. However, when we look at the Q-Q plot, we see that the plot does not fully follow the reference line from -2 to 2 on the Theoretical Quantiles axis. This is proven by Shapiro-Wilk Test. With a p-value of 3.072e-14, we reject the null hypothesis and state that the distribution is not normal. Therefore, we need to transform our response variable. Below, we will try a few transformationts to see if that makes the distribution more normal.

Square Root Transformation:
```{r 18}
# Square Root
# histogram
hist(sqrt(cov_df$total_anxiety_level),
     xlab = "Square Root(Total Anxiety Level)",
     main = "Square Root of Total Anxiety Level Distribution")

# q-q plot
qqnorm(sqrt(cov_df$total_anxiety_level), main = "Q-Q Plot of sqrt(Total Anxiety Level)")
qqline(sqrt(cov_df$total_anxiety_level), col = "red")


# Shapiro-Wilk Test
print(shapiro.test(sqrt(cov_df$total_anxiety_level)))

# DJ
```
Based on the histogram, Q-Q plot, and the Shapiro-Wilk Test, the square root transformation did not make the distribution more normal.

Squared:
```{r 19}
# Squared
# histogram
hist((cov_df$total_anxiety_level)^2,
     xlab = "(Total Anxiety Level)^2",
     main = "Total Anxiety Level Squared Distribution")

# q-q plot
qqnorm((cov_df$total_anxiety_level)^2, main = "Q-Q Plot of (Total Anxiety Level)^2")
qqline((cov_df$total_anxiety_level)^2, col = "red")


# Shapiro-Wilk Test
print(shapiro.test((cov_df$total_anxiety_level)^2))

# DJ
```
Based on the histogram, Q-Q plot, and the Shapiro-Wilk Test, the squared transformation did not make the distribution more normal.

Log Transformation:
```{r 20}
# Log
# histogram
hist(log(cov_df$total_anxiety_level),
     xlab = "log(Total Anxiety Level)",
     main = "Log Total Anxiety Level Distribution")

# DJ
```

Just looking at the log transformation histogram, we see that the transformation did not make the response variable more normal.

Let us try the Box-Cox transformation:
```{r 21}
# Box-Cox transformation from MASS package
# First, we need to ensure our response variable is positive before transforming.
range(cov_df$total_anxiety_level)
# 0 to 500 is our range.
# Therefore, we need to shift data to ensure it is all positive. We will shift by 1.
cov_df$total_anxiety_level_shift <- cov_df$total_anxiety_level - min(cov_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = cov_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# Optimal lambda is 1.1

# Box-Cox transformation function
boxcox_transform <- function(x, lambda) {
  if (lambda == 0) {
    return(log(x))  # special case for when lambda = 0
  } else {
    return((x^lambda - 1) / lambda)
  }
}

# Now we apply transformation to total_anxiety_level_shift
cov_df$box_cox_anxiety_level <- boxcox_transform(cov_df$total_anxiety_level_shift, lambda_opt)

# histogram
hist(cov_df$box_cox_anxiety_level,
     xlab = "Box-Cox Total Anxiety Level",
     main = "Box-Cox Total Anxiety Level Distribution")

qqnorm(cov_df$box_cox_anxiety_level, main = "Q-Q Plot of Box-Cox Total Anxiety Level")
qqline(cov_df$box_cox_anxiety_level, col = "red")


# Shapiro-Wilk Test
print(shapiro.test(cov_df$box_cox_anxiety_level))

# DJ
```
After our Box-Cox transformation, we see that our p-value of the Shapiro-Wilk Test is still not greater than 0.05. However, after our Box-Cox transformation, the p-value is much closer to 0.05 than our initial p-value. Therefore, since the p-value is closer to 0.05, we should use a Box-Cox transformation on our response variable for both our training and our test data sets.
```{r 22}
orig_shapiro = shapiro.test(cov_df$total_anxiety_level)
bc_shapiro = shapiro.test(cov_df$box_cox_anxiety_level)

orig_p_val = orig_shapiro$p.value
bc_p_val = bc_shapiro$p.value

print(paste("Original p-value:", orig_p_val))
print(paste("Box-Cox transformed p-value:", bc_p_val))

if (bc_p_val > orig_p_val) {
  print("Yes, p-value after Box-Cox transformation is greater than original p-value")
} else {
  print("No, p-value after Box-Cox transformation is not greater than original p-value")
}

# Now, let us Box-Cox transform our response variable in our training and test data
# Training
range(train_df$total_anxiety_level)
# 0 to 500
train_df$total_anxiety_level_shift <- train_df$total_anxiety_level - min(train_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = train_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# 1.1 again
train_df$box_cox_anxiety_level <- boxcox_transform(train_df$total_anxiety_level_shift, lambda_opt)

# Now, let us remove total_anxiety_level and total_anxiety_level_shift from train_df then rename the box_cox_anxiety_level to total_anxiety_level. Let us also round to the nearest whole number
train_df <- train_df %>%
  dplyr::select(-total_anxiety_level, -total_anxiety_level_shift) %>%
  dplyr::rename(total_anxiety_level = box_cox_anxiety_level) %>%
  dplyr::mutate(total_anxiety_level = round(total_anxiety_level))

# Test Data
range(test_df$total_anxiety_level)
# 0 to 500
test_df$total_anxiety_level_shift <- test_df$total_anxiety_level - min(test_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = test_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# 1.1 again
test_df$box_cox_anxiety_level <- boxcox_transform(test_df$total_anxiety_level_shift, lambda_opt)

# Now, let us remove total_anxiety_level and total_anxiety_level_shift from train_df then rename the box_cox_anxiety_level to total_anxiety_level. Let us also round to the nearest whole number
test_df <- test_df %>%
  dplyr::select(-total_anxiety_level, -total_anxiety_level_shift) %>%
  dplyr::rename(total_anxiety_level = box_cox_anxiety_level) %>%
  dplyr::mutate(total_anxiety_level = round(total_anxiety_level))

# DJ
```

Now that we have properly transformed our response variable, we can now center and scale our predictors to prepare for PCA.

# Center and Scale Predictors
```{r 23}
length(levels(train_df$age))
length(levels(test_df$age))
# 6 levels

length(levels(train_df$source))
length(levels(test_df$source))
# 7 levels

length(levels(train_df$ethnoracial_group))
length(levels(test_df$ethnoracial_group))
# 4 levels

# DJ
```
We see that age has 6 unique classifiers, source has 7 unique classifiers and ethnoracial_group has 4 levels. Since we will be doing a PCA to reduce our dimensionality of our data set, let us one hot encode all of the categorical predictors, then scale/center them.

```{r 24}
# Creating dummy variables (one-hot-encoding) for the specified columns in train_df and test_df

# train_df
onehot_encoder <- dummyVars(~ age + source + ethnoracial_group,
                            train_df[, c("age", "source",
                                         "ethnoracial_group")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
# Predicting dummy variables using the onehot_encoder for train_df
onehot_enc_train <- predict(onehot_encoder,
                            train_df[,c("age", "source",
                                        "ethnoracial_group")])
# Combining the original train_df with the one-hot encoded variables
train_df <- cbind(train_df, onehot_enc_train)

# Snake case train_df
#using the Janitor Package for snake casing each var
train_df <- train_df %>%
  clean_names(case = "snake")

# Remove three variables that were one-hot encoded:
# age, source, ethnoracial_group
feats_to_remove <- c("age", "source", "ethnoracial_group")
train_df <- train_df %>%
  dplyr::select(-feats_to_remove)


# test_df
# Creating dummy variables (one-hot-encoding) for the specified columns in test_df
onehot_encoder <- dummyVars(~ age + source + ethnoracial_group,
                            test_df[, c("age", "source",
                                         "ethnoracial_group")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
# Predicting dummy variables using the onehot_encoder for test_df
onehot_enc_test <- predict(onehot_encoder,
                            test_df[,c("age", "source",
                                        "ethnoracial_group")])
# Combining the original test_df with the one-hot encoded variables
test_df <- cbind(test_df, onehot_enc_test)

# Snake case train_df
# Using the Janitor Package for snake casing each var
test_df <- test_df %>%
  clean_names(case = "snake")

# Remove three variables that were one-hot encoded:
# age, source, ethnoracial_group
feats_to_remove <- c("age", "source", "ethnoracial_group")
test_df <- test_df %>%
  dplyr::select(-feats_to_remove)

# Before centering and scaling. Check to see if there are any columns that add up to zero in test_df

# train_df
colSums(train_df)
column_sums <- colSums(train_df)
# Print the column sums
print(column_sums)
# Identify columns with a sum of zero
zero_sum_columns <- names(column_sums[column_sums == 0])
print(zero_sum_columns)
# Returns 0. No zero columns.

# test_df
colSums(test_df)
column_sums <- colSums(test_df)
# Print the column sums
print(column_sums)
# Identify columns with a sum of zero
zero_sum_columns <- names(column_sums[column_sums == 0])
print(zero_sum_columns)
# Returns "65_to_74"

# Let us remove that variable from both train_df and test_df so we do not run into any issues when scaling/centering 
feats_to_remove = c("x65_to_74")
train_df <- train_df %>%
  dplyr::select(-feats_to_remove)

test_df <- test_df %>%
  dplyr::select(-feats_to_remove)

# DJ/NT
```

Now we can center and scale our predictors.
```{r 25}
# train_df
# Storing the response variable into a vector (total_anxiety_level)
train_resp <- train_df$total_anxiety_level

# Creating a train_df without response variable
feats_to_remove <- c("total_anxiety_level")
train_df <- train_df %>%
  dplyr::select(-feats_to_remove)

train_feat <- scale(train_df, center = TRUE, scale = TRUE)
apply(train_feat, 2, sd)

train_feat <- as.data.frame(train_feat)
train_resp <- as.data.frame(train_resp)
train_df = cbind(train_feat, train_resp)

# test_df
# Storing the response variable into a vector (total_anxiety_level)
test_resp <- test_df$total_anxiety_level

# Creating a train_df without response variable
feats_to_remove <- c("total_anxiety_level")
test_df <- test_df %>%
  dplyr::select(-feats_to_remove)

test_feat <- scale(test_df, center = TRUE, scale = TRUE)
apply(test_feat, 2, sd)

test_feat <- as.data.frame(test_feat)
test_resp <- as.data.frame(test_resp)
test_df = cbind(test_feat, test_resp)

# DJ
```

# PCA (principal component analysis) data reduction
```{r 26}
# Perform PCA
pca_train <- prcomp(train_feat, center = FALSE, scale. = FALSE)
# predictors already trained and scaled

# View PCA summary
summary(pca_train)

# Look at scree plot
eigenvalues <- pca_train$sdev^2

# Compute the proportion of variance explained by each component
variance_explained <- eigenvalues / sum(eigenvalues)

# Plot manually
plot(variance_explained, type = "b", main = "Scree Plot", xlab = "Principal Component", ylab = "Proportion of Variance Explained")

# DJ
```

Looks like we have a few elbow that we can point to in our scree plot. We see the proportion of variance explained tend to level out after the 5th, 10th, 15th, 19th, and 25th principal components. The drop off in proportion of variance explained is relatively small when comparing those principal components. However, the proportion of variance explained really seems to level off after the 25th principal component. In that case, let us change the scaled trained data set to a new data set with those 25 principal components and add the response variable as well. Let us convert that new data set to a data frame.

```{r 27}
pca_scores <- pca_train$x[, 1:25]
# Combine the reduced principal components with the response variable
reduced_train_df <- cbind(pca_scores, train_resp)
# Convert to data frame
reduced_train_df = as.data.frame(reduced_train_df)

# DJ
```

Now let us do the same for the test model.
```{r 28}
# Perform PCA
pca_test <- prcomp(test_feat, center = FALSE, scale. = FALSE)
# predictors already trained and scaled

# View PCA summary
summary(pca_test)

# Look at scree plot
eigenvalues <- pca_test$sdev^2

# Compute the proportion of variance explained by each component
variance_explained <- eigenvalues / sum(eigenvalues)

# Plot manually
plot(variance_explained, type = "b", main = "Scree Plot", xlab = "Principal Component", ylab = "Proportion of Variance Explained")

pca_scores_test <- pca_test$x[, 1:25]
# Combine the reduced principal components with the response variable
reduced_test_df <- cbind(pca_scores_test, test_resp)
# Convert to data frame
reduced_test_df = as.data.frame(reduced_test_df)

# DJ
```

Now we can start fitting our first model!

# Intial model building 
## Multiple Linear Regression Model
Let us first just try a simple linear regression model using backwards selection. We will start by fitting a model on the reduced training data frame. For any predictor that has a p-value greater than 0.05, we will remove that predictor. We will go one by one and remove non-statistically significant predictor with the highest p-value, then re-run our model.
```{r lm 1}
set.seed(123)
cov_lm = lm(train_resp ~., reduced_train_df)
summary(cov_lm)

# DJ
```

Let us remove PC16 first.
```{r lm 2}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC18
```{r lm 3}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC13
```{r lm 4}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC25
```{r lm 5}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC15
```{r lm 6}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC17
```{r lm 7}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15 - PC17, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC19
```{r lm 8}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15 - PC17 - PC19, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC11
```{r lm 9}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15 - PC17 - PC19 - PC11, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC20
```{r lm 10}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15 - PC17 - PC19 - PC11 - PC20, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC14
```{r lm 11}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15 - PC17 - PC19 - PC11 - PC20 - PC14, reduced_train_df)
summary(cov_lm)

# DJ
```

Remove PC24
```{r lm 12}
set.seed(123)
cov_lm = lm(train_resp ~. - PC16 - PC18 - PC13 - PC25 - PC15 - PC17 - PC19 - PC11 - PC20 - PC14 - PC24, reduced_train_df)
summary(cov_lm)

# DJ
```

```{r}
#Diagnostic Plots for multiple linear regression
par(mfrow=c(2,2))
plot(cov_lm)

#NT
```


Now, all of the p-values for each principal component are less than 0.05. Let us test the mean squared error rate on the training data itself.
```{r lm 13}
set.seed(123)
pred_train = predict(cov_lm, newdata = reduced_train_df)
mean((pred_train - reduced_train_df$train_resp)^2)

# Plot predicted versus actual 
plot(pred_train, reduced_train_df$train_resp,
     xlab = "Predicted Values",
     ylab = "Actual Values",
     main = "Scatter Plot of Predicted Training Values vs Actual Values",
     pch = 16, 
     cex = 0.5,
     col = "black") 
lm_model <- lm(reduced_train_df$train_resp ~ pred_train)
abline(lm_model, col = "red", lwd = 2)

# DJ
```

Now try on test data
```{r lm 14}
set.seed(123)
pred_test = predict(cov_lm, newdata = reduced_test_df)
mean((pred_test - reduced_test_df$test_resp)^2)

plot(pred_test, reduced_test_df$test_resp,
     xlab = "Predicted Values",
     ylab = "Actual Values",
     main = "Scatter Plot of Predicted Test Values vs Actual Values",
     pch = 16, 
     cex = 0.5,
     col = "black") 
lm_model <- lm(reduced_train_df$train_resp ~ pred_train)
abline(lm_model, col = "red", lwd = 2)

# DJ
```

We see a very high mean squared error rate for both the training and test mean squared error rates. 

## Regression Tree
```{r rt 1}
set.seed(123)
cov_tree = tree(train_resp ~ ., reduced_train_df)
summary(cbm_tree)
plot(cbm_tree)
text(cbm_tree, pretty = 0)

yhat = predict(cov_tree, newdata = reduced_train_df)
mean((yhat - reduced_train_df$train_resp)^2)

yhat = predict(cov_tree, newdata = reduced_test_df)
mean((yhat - reduced_test_df$test_resp)^2)

# DJ
```

## Lasso Regression
```{r lasson 1}
# Perform lasso regression
set.seed(123)
train_matrix = model.matrix(train_resp~., reduced_train_df)
test_matrix = model.matrix(test_resp~., reduced_test_df)
# Select lambda using cross-validation
cv_lam = cv.glmnet(train_matrix, reduced_train_df$train_resp, alpha = 1)
best_lam = cv_lam$lambda.min
best_lam
# Lambda chosen by cross-validation is 1.074
# Now we fit lasso regression model and predict train data
cov_lasso = glmnet(train_matrix, reduced_train_df$train_resp, alpha = 1)
pred_lasso = predict(cov_lasso, s = best_lam, newx = train_matrix)
# Find training error:
mean((pred_lasso - reduced_train_df$train_resp)^2)

# Now we fit lasso regression model and predict test data
pred_lasso = predict(cov_lasso, s = best_lam, newx = test_matrix)
# Find training error:
mean((pred_lasso - reduced_test_df$test_resp)^2)

# DJ
```

# Neural Network Model
Any (NN) next to the code block number means for neural network data cleaning/pre-processing steps

We are reading the data in again and performing the same data cleaning and pre-processing that we did above for quantitative regression. We did not need to perform PCA here as a neural network is great at reducing dimensionality.

## Fit Model
```{r nn 1}
set.seed(123)
calcSplitRatio(37, train_df)

train_feat <- as.matrix(train_feat)
train_resp <- as.matrix(train_resp)
test_feat <- as.matrix(test_feat)
test_resp <- as.matrix(test_resp)

use_virtualenv("my_tf_workspace")

model <- keras_model_sequential(list(
  layer_dense(units = 75, activation = "relu"),
  layer_dense(units = 50, activation = "relu"),
  layer_dense(units = 25, activation = "relu"),
  layer_dense(units = 1, activation = "linear")
))

compile(model,
        optimizer = "rmsprop",
        loss = "mean_squared_error",
        metrics = "mean_squared_error")

early_stopping <- callback_early_stopping(patience = 5)

history <- fit(model, train_feat, train_resp,
               epochs = 50, 
               batch_size = 512, 
               validation_split = 0.16,
               callbacks = list(early_stopping))
plot(history)

# Training Error Rate
train_results = model %>%
  evaluate(train_feat, train_resp)
print(train_results)

# Test Error Rate
test_results = model %>%
  evaluate(test_feat, test_resp)
print(test_results)

# NT/DJ
```

Due to the randomness of neural network models, we will get slightly different results every time we fit/test our neural network model. After our most recent model fitting, we were able to produce error rates of:
Training MSE: 748.92
Test MSE: 807.09


# Fine-Tune Model
Let us know try adjusting our model to seee if we can produce a lower training and test MSE all while avoiding overfitting. We will add dropout layers to our model. Dropout layers are a regularization technique to prevent overfitting. We will also try a different optimizer in this model. The Adam optimzer integrates both momentum and RMSprop into one algorithm. WE will use learning rate of 0.001. We will up our patience form 5 epochs to 10, meaning that the model will notice if there is no improvement in the MSE of the mode after 10 epochs. If there is no improvement, we will add code to adjust the learning rate. We will also add more epochs and batch size to try and over fit and have the model respond to the overfitting

```{r nn 2}
set.seed(123)

# Add the drop out layers
model_1 <- keras_model_sequential(list(
  layer_dense(units = 75, activation = "relu", input_shape = ncol(train_feat)),
  layer_dropout(rate = 0.5),
  layer_dense(units = 50, activation = "relu"),
  layer_dropout(rate = 0.5),
  layer_dense(units = 25, activation = "relu"),
  layer_dense(units = 1, activation = "linear")
))

# Compile the model with a different optimizer and learning rate
compile(model_1,
        optimizer = optimizer_adam(lr = 0.001), # changed learning rate to be very small
        loss = "MeanSquaredError",
        metrics = "MeanSquaredError")

# Callbacks: Early stopping and learning rate reduction
callbacks <- list(
  callback_early_stopping(patience = 10),
  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.2, patience = 5)
)

# Train the model
history_1 <- fit(model_1, train_feat, train_resp,
               epochs = 100,  # Try more epochs
               batch_size = 256,  # Adjust batch size
               validation_split = 0.16,
               callbacks = callbacks)

# Plot the training history
plot(history_1)

# DJ
```

```{r nn 3}
# Training Error Rate
train_results_1 = model_1 %>%
  evaluate(train_feat, train_resp)
print(train_results_1)

# Test Error Rate
test_results_1 = model_1 %>%
  evaluate(test_feat, test_resp)
print(test_results_1)

# DJ
```
Our fine tuned neural network model was able to produce a smaller training and test MSE as our first one. Still are incredibly high.

Training MSE: 841.92
Test MSE: 866.96

However, there were much more intricate layers to the model in our second neural network model. Since it produced a similar training and test MSE, we recommend the first model as it is less intricate as the model that we fine tuned. 

Our neural network model, although more difficult to convey to stakeholders, produced a model that performed much better than our linear regression model. We were able to get our test MSE from 10848.07 in our linear regression model down to 0.17 in our neural network model. 

We want to keep our stakeholders in mind here as they may not have the technical skills to interpret a neural network model. We could ensure them that the model finds underlying linear patterns and linear relationships in our data that the linear regression model cannot find. However, they may be skeptical as there are no specific predictor that we can point to in our neural network model.

For our final model, we will fit a Support Vector Machine model. We want to try and fit a similar performing model that may be more interpretable than our neural network model. Even if we produce similar results, if we can convey the process and identify variables with more predictive power to our stakeholders, it will be much more easier to implement our policies to help students who suffer from COVID-19 anxiety.

# Support Vector Machine Model
 The goal of a Support Vector Machine model is to find a function that approximates the data within a certain error threshold and aims to be as flat as possible while deviating from the actual data points by no more than the error threshold. First need to convert training and test data back to data frame.
```{r svm 1}
train_feat <- as.data.frame(train_feat)
train_resp <- as.data.frame(train_resp)

test_feat <- as.data.frame(test_feat)
test_resp <- as.data.frame(test_resp)


# SVM model
# Fit SVM model for regression
set.seed(123)
svm_model <- svm(train_feat, train_resp, 
                  type = "eps-regression", 
                  kernel = "radial",        
                  cost = 1,                
                  epsilon = 0.5) 
summary(svm_model)

# Train MSE
predictions <- predict(svm_model, train_feat)
predictions <- as.data.frame(predictions)
train_mse <- mean((predictions[,1] - train_resp[,1])^2)
print(paste("Train MSE:", round(train_mse, 2)))


# Test MSE
predictions <- predict(svm_model, test_feat)
predictions <- as.data.frame(predictions)
test_mse <- mean((predictions[,1] - test_resp[,1])^2)
print(paste("Test MSE:", round(test_mse, 2)))

# DJ
```

Returned Train MSE of 403 and Test MSE of 655.

Can we fine-tune our SVM model?

Understand Hyperparameters:

cost: Controls the trade-off between achieving a low error on the training data and minimizing the model complexity.

epsilon: Defines the margin of tolerance where no penalty is applied for errors in SVR.

kernel: The kernel function transforms the input space into a higher-dimensional space. Common kernels are "linear", "polynomial", and "radial".

degree: For polynomial kernels, the degree of the polynomial.

gamma: determines shape of decision boundary.

Let us find optimal values of cost, epsilon and gamma in this model. Let us tune those hyper parameters on 10% of data to avoid long code processing time.
```{r svm 2}
set.seed(123)
svm_model <- svm(train_feat, train_resp, 
                  type = "eps-regression", 
                  kernel = "radial",        
                  cost = 1,                
                  epsilon = 0.5)
summary(svm_model)

# Train MSE
predictions <- predict(svm_model, train_feat)
predictions <- as.data.frame(predictions)
train_mse <- mean((predictions[,1] - train_resp[,1])^2)
print(paste("Train MSE:", round(train_mse, 2)))


# Test MSE
predictions <- predict(svm_model, test_feat)
predictions <- as.data.frame(predictions)
test_mse <- mean((predictions[,1] - test_resp[,1])^2)
print(paste("Test MSE:", round(test_mse, 2)))

# DJ
```

Fine tune SVM model to find best cost, epsilon, gamma parameters.
```{r svm 3}
# Define the task
task <- makeRegrTask(data = as.data.frame(cbind(train_feat, train_resp)), target = "train_resp")

# Define the learner
learner <- makeLearner("regr.svm", kernel = "radial")

# Define the parameter space
ps <- makeParamSet(
  makeNumericParam("cost", lower = 0.1, upper = 100),
  makeNumericParam("epsilon", lower = 0.1, upper = 0.4),
  makeNumericParam("gamma", lower = 0.1, upper = 10)
)

# Define the performance measure for regression
measure <- mse

# Define the tuning method
tune_ctrl <- makeTuneControlGrid(resolution = 5)

# Define resampling method
resampling <- makeResampleDesc("CV", iters = 5)

# Perform the tuning
tune_result <- tuneParams(
  learner, 
  task, 
  resampling, 
  tune_ctrl, 
  par.set = ps,
  measures = measure
)

# Print the best parameters
print(tune_result$x)

# cost
# 75.025

# epsilon
# 0.4

# gamma
# 0.1

# DJ
```

Fit SVM model with optimal parameters
```{r svm 4}
set.seed(123)
svm_model <- svm(train_feat, train_resp, 
                  type = "eps-regression", 
                  kernel = "radial",        
                  cost = 75.25,                
                  epsilon = 0.4,
                 gamma = 0.1)
summary(svm_model)

# Train MSE
predictions <- predict(svm_model, train_feat)
predictions <- as.data.frame(predictions)
train_resp <- as.data.frame(train_resp)
train_mse <- mean((predictions[,1] - train_resp[,1])^2)
print(paste("Train MSE:", round(train_mse, 2)))


# Test MSE
predictions <- predict(svm_model, test_feat)
predictions <- as.data.frame(predictions)
test_mse <- mean((predictions[,1] - test_resp[,1])^2)
print(paste("Test MSE:", round(test_mse, 2)))

# Training MSE of 127
# Test MSE of 761

# DJ
```

Try with CV = 10
```{r svm 5}
library(mlr)

# Define the task
task <- makeRegrTask(data = as.data.frame(cbind(train_feat, train_resp)), target = "train_resp")

# Define the learner
learner <- makeLearner("regr.svm", kernel = "radial")

# Define the parameter space
ps <- makeParamSet(
  makeNumericParam("cost", lower = 0.1, upper = 100),
  makeNumericParam("epsilon", lower = 0.1, upper = 0.4),
  makeNumericParam("gamma", lower = 0.1, upper = 10)
)

# Define the performance measure for regression
measure <- mse

# Define the tuning method
tune_ctrl <- makeTuneControlGrid(resolution = 5)

# Define resampling method
resampling <- makeResampleDesc("CV", iters = 10)

# Perform the tuning
tune_result <- tuneParams(
  learner, 
  task, 
  resampling, 
  tune_ctrl, 
  par.set = ps,
  measures = measure
)

# Print the best parameters
print(tune_result$x)

# cost
# 50.05

# epsilon
# 0.4

# gamma
# 0.1

# DJ
```

```{r svm 6}
set.seed(123)
svm_model <- svm(train_feat, train_resp, 
                  type = "eps-regression", 
                  kernel = "radial",        
                  cost = 50.05,                
                  epsilon = 0.4,
                 gamma = 0.1)
summary(svm_model)

# Train MSE
predictions <- predict(svm_model, train_feat)
predictions <- as.data.frame(predictions)
train_resp <- as.data.frame(train_resp)
train_mse <- mean((predictions[,1] - train_resp[,1])^2)
print(paste("Train MSE:", round(train_mse, 2)))


# Test MSE
predictions <- predict(svm_model, test_feat)
predictions <- as.data.frame(predictions)
test_mse <- mean((predictions[,1] - test_resp[,1])^2)
print(paste("Test MSE:", round(test_mse, 2)))

# Training MSE: 127.69
# Test MSE: 761.54

# DJ
```

This produced the same training and test MSE as the previous SVM model. Let use the the model with less cross validations for faster computing time.

```{r}
summary(svm_model)
```

# Throwaway Code
Typically anything that is above the ((75th quartile) + 1.5 * interquartile range (IQR)) or below ((75th quartile) - 1.5 * interquartile range (IQR)) are potential outliers and are identified by the boxplot.stats function. The variables "age," "hrs_outdoor," "hrs_exercise," are qualitative variables so we won't strictly adhere to these outliers identified by the boxplox.stats function. As for "bmi," this is a quantitative continuous variable and will use this function identify the potential outliers.
```{r throw 1}
#this will remove replace all of the potential outliers with N/a values
for (x in c("bmi"))
{
  value = cov_df[,x][cov_df[,x] %in% boxplot.stats(cov_df[,x])$out]
  cov_df[,x][cov_df[,x] %in% value] = NA
}
#This will remove all of the missing values from the columns
cov_df <- drop_na(cov_df)
#checking to see if the N/a values are removed from the data set
colSums(is.na(cov_df))

# NT
```

I thought there there were a few observations from the qualitative variables that I would consider as outliers and wanted to get rid of them in the case that they will affect our models in a bad way.
```{r throw 2}
#creating a cut off for hours of exercise and then subsetting those that aren't wanted from the data frame
threshold_hrs_exercise <- 9 
cov_df <- subset(cov_df, hrs_exercise < threshold_hrs_exercise)
#creating an array of which ages that should be kept in the data frame
ages_to_keep <- c("18 to 24", "25 to 32", "33 to 44", "45 to 54", "55 to 64")
cov_df <- filter(cov_df, age %in% ages_to_keep)

# NT
```

```{r throw 3}
#performing 
pca_test <- prcomp(scaled_test_df)
str(pca_test)

library(factoextra)
fviz_eig(pca_test, addlabels = TRUE, ylim = c(0,100))

# NT
```

## Ordinal Logistic Regression
For our first model, we are going to build an ordinal logistic regression model. For that we need to first turn our response variable into ordinal. 

Firsts lets see the spread of of our variable. 
```{r throw 4}
summary(reduced_train_df$train_resp)

#NP
```

```{r throw 5}
# Its better to visualize using a histogram. Lets plot a basic one. 

hist(reduced_train_df$train_resp,
     main = "Histogram of Total Anxiety Level",
     xlab = "Total Anxiety Level",
     ylab = "Frequency",
     col = "skyblue",
     border = "black",
     breaks = 30)  
#NP
```
To turn total_anixety_level we need to first break it into intervals and then create the ordinal variable. 

```{r throw 6}
# the breaks will be from 0-100, 101-199, 200-299 and so forth,

breaks <- c(-Inf, 100, 199, 299, 399, 499, 599, 699, 799, Inf)
labels <- 1:9

# And now lets put them into the variable 
reduced_train_df$or_train_resp<- cut(reduced_train_df$train_resp,
                                        breaks = breaks,
                                        labels = labels,
                                        right = TRUE)

#this code shows weather or not it works
head(reduced_train_df$or_train_resp)

# NP
```

```{r throw 7}
# now lets do the same with our test variable
breaks <- c(-Inf, 100, 199, 299, 399, 499, 599, 699, 799, Inf)
labels <- 1:9

reduced_test_df$or_test_resp <- cut(reduced_test_df$test_resp,
                                        breaks = breaks,
                                        labels = labels,
                                        right = TRUE)
head(reduced_test_df$or_test_resp)
#NP
```

```{r throw 8}
#this code is redundant but it ensures that both of the data sets variables are ordinal 

reduced_train_df$or_train_resp <- factor(reduced_train_df$or_train_resp,
                                          levels = 1:9,
                                          ordered = TRUE)


reduced_test_df$or_test_resp <- factor(reduced_test_df$or_test_resp,
                                          levels = 1:9,
                                          ordered = TRUE)
#NP
```

```{r throw 9}
#now lets see the plot for the distribution of the variable 

ggplot(reduced_train_df, aes(x = or_train_resp)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Ordinal Anxiety Levels",
       x = "Ordinal Anxiety Level",
       y = "Count") +
  theme_minimal()
#NP
```


```{r throw 10}
#and now some test stats
table(reduced_train_df$or_train_resp)
table(reduced_test_df$or_test_resp)

summary(reduced_train_df$or_train_resp)
summary(reduced_test_df$or_test_resp)

#NP
```

Now let us fit an ordinal logistic regression model
```{r throw 11}
cov_or <- polr(or_train_resp ~. -train_resp, data = reduced_train_df, Hess = TRUE)
summary(cov_or)

# NP
```


```{r throw 12}
# Creating dummy variables (one-hot-encoding) for the specified columns in train_df
# train_df
onehot_encoder <- dummyVars(~ age + source + ethnoracial_group,
                            train_df[, c("age", "source",
                                         "ethnoracial_group")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
# Predicting dummy variables using the onehot_encoder for train_df
onehot_enc_train <- predict(onehot_encoder,
                            train_df[,c("age", "source",
                                        "ethnoracial_group")])
# Combining the original train_df with the one-hot encoded variables
train_df <- cbind(train_df, onehot_enc_train)

# Creating dummy variables (one-hot-encoding) for the specified columns in test_df
onehot_encoder <- dummyVars(~ age + source + ethnoracial_group,
                            test_df[, c("age", "source",
                                         "ethnoracial_group")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
# Predicting dummy variables using the onehot_encoder for test_df
onehot_enc_test <- predict(onehot_encoder,
                            test_df[,c("age", "source",
                                        "ethnoracial_group")])
# Combining the original test_df with the one-hot encoded variables
test_df <- cbind(test_df, onehot_enc_test)

# Specifying the columns to exclude from data sets
cols_to_exclude <- c(22, 23, 24, 26)

# Scale the test_df columns (excluding specified columns) using mean and sd from train_df
test_df[, -(cols_to_exclude)] <- scale(test_df[, -c(cols_to_exclude)],
                              center = apply(train_df[, -c(cols_to_exclude)], 2, mean),
                              scale = apply(train_df[, -c(cols_to_exclude)], 2, sd))
# Scale the train_df columns (excluding specified columns)
train_df[, -c(cols_to_exclude)] <- scale(train_df[, -c(cols_to_exclude)])

# Choosing the amount of features to be included in the neural network
training_features <- array(data = unlist(train_df[, -c(cols_to_exclude)]),
                                         dim = c(nrow(train_df), 37))
# Choosing the response variable from the train_df
training_labels <- array(data = unlist(train_df[, 25]),
                         dim = c(nrow(train_df)))
# Choosing the amount of features to be included in the neural network
test_features <- array(data = unlist(test_df[, -c(cols_to_exclude)]),
                       dim = c(nrow(test_df), 37))
# Choosing the response variable from the test_df
test_labels <- array(data = unlist(test_df[, 25]),
                     dim = c(nrow(test_df)))

# Excluding the original columns that were one-hot-encoded
train_df <- train_df[, !names(train_df) %in% c ("age", "source", "ethnoracial_group")]
test_df <- test_df[, !names(test_df) %in% c ("age", "source", "ethnoracial_group")]

# NT
```

```{r throw 13}
#training data

#storing the response variable into a vector (total_anxiety_level)
anx_lev_train <- train_df$total_anxiety_level

#creating a scaled_train_df without response variable
scaled_train_df <- train_df[,-29]

#centering and scaling using standard deviation of 2
scaled_train_df <- scale(scaled_train_df, center = TRUE, scale = TRUE)
apply(scaled_train_df, 2, sd)

#adding response variable back in scaled df
scaled_train_df <- cbind(scaled_train_df, anx_lev_train)

#test data

#storing the response variable into a vector (total_anxiety_level)
anx_lev_test <- test_df$total_anxiety_level

#creating a scaled_test_df without response variable
scaled_test_df <- test_df[,-29]

#centering and scaling using standard deviation of 2
scaled_test_df <- scale(scaled_test_df, center = TRUE, scale = TRUE)
apply(scaled_test_df, 2, sd)

#adding response variable back in scaled df
scaled_test_df <- cbind(scaled_test_df, anx_lev_test)

# NT
```

## Re-read in data
```{r throw nn 1}
cov_df <- read.csv("capstone_data.csv")

# Remove X variable, ResponseId variable
cov_df <- cov_df[, -which(names(cov_df) == "X")]
cov_df <- cov_df[, -which(names(cov_df) =="ResponseId")]

# We want to use snakecase on features and simplify features. Before we do that, we need to figure out what features we want to keep and what ones we want to get rid of.
# We already got rid of the X variable from converting our file to a CSV and the ResponseID.

# Data frame dimension
dim(cov_df)
# 2534 observations
# 57 variables

# Print names of variables in data set
colnames(cov_df)

# Check for NA values
colSums(is.na(cov_df))

# DJ
```
```{r throw nn 2}
cov_df <- cov_df[, -which(names(cov_df) =="Class_Self_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Class_Mom_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Class_Dad_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Income_Relative_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_Mom_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_Dad_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Health_General_group")]
cov_df <- cov_df[, -which(names(cov_df) =="BMI_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Outdoor_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Screen_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Hrs_Exercise_group")]
cov_df <- cov_df[, -which(names(cov_df) =="Age_18to25")]
cov_df <- cov_df[, -which(names(cov_df) =="Age_3Groups")]
cov_df <- cov_df[, -which(names(cov_df) =="Source_NCSU")]
cov_df <- cov_df[, -which(names(cov_df) =="GIS_ZIP")]
cov_df <- cov_df[, -which(names(cov_df) =="COVID_F1_BadMoods")]
cov_df <- cov_df[, -which(names(cov_df) =="COVID_F2_LostTime")]

# Check for NAs again
colSums(is.na(cov_df))

# 280 NA values in Ethnoracial_Group, NHWhite_f, Ethnoracial_Group_White1_Asian2 
# 8 NA values in Age and Source

# Store Classification into empty vector for now. Might need to revisit this later and re-read in data if remove any observations. And then remove Classification and Classification_High
psycho_class <- c()
pyscho_class <- cov_df$Classification
cov_df <- cov_df[, -which(names(cov_df) =="Classification")]
cov_df <- cov_df[, -which(names(cov_df) =="Classification_High")]

# Check for NAs again
colSums(is.na(cov_df))

# Dimension
dim(cov_df)
# 2534 observations
# 38 variables

# DJ
```
```{r throw nn 3}
# Which features do we want to round up: COVID_Afraid, COVID_Irritable, COVID_Guilty, COVID_Sad, COVID_Preoccupied and COVID_Stressed
features_round_up <- c("COVID_Afraid", "COVID_Irritable", "COVID_Guilty", "COVID_Sad", "COVID_Preoccupied", "COVID_Stressed")
cov_df <- cov_df %>%
  mutate_at(vars(features_round_up), ~ ceiling(.))

# Now let us merge these into one variable called total_anxiety_level
cov_df <- cov_df %>%
  mutate(total_anxiety_level = COVID_Afraid + COVID_Irritable + 
           COVID_Guilty + COVID_Sad + COVID_Preoccupied)

# Now, let us remove those variables added together from the data set
cov_df <- cov_df %>%
  dplyr::select(-any_of(features_round_up))

# Find average and median value of total_anxiety_level
avg_tot_anx_lev <- mean(cov_df$total_anxiety_level)
# Mean is 251.17
med_tot_anx_lev <- median(cov_df$total_anxiety_level)
# Median is 260

dim(cov_df)
# 2534 observations
# 33 Variables

# DJ
```
```{r throw nn 4}
cov_df <- cov_df %>%
  mutate_if(is.numeric, function(x) round(x))

# We do not have enough information from out data on the Type variable. Let us remove that.
# Also, the binary Educ_College_Grad can be derived from Educ_Self variable. Let us remove that as well.
# Since we already have a Ethnoracial_Group variable, we can remove Ethnoracial_Group_f, NHWhite_f and Ethnoracial_Group_White1_Asian2 
cov_df <- cov_df[, -which(names(cov_df) =="Type")]
cov_df <- cov_df[, -which(names(cov_df) =="Educ_College_Grad")]
cov_df <- cov_df[, -which(names(cov_df) =="Ethnoracial_Group_f")]
cov_df <- cov_df[, -which(names(cov_df) =="NHWhite_f")]
cov_df <- cov_df[, -which(names(cov_df) =="Ethnoracial_Group_White1_Asian2")]

dim(cov_df)
# 2534 observations
# 28 variables

# Check for NAs again
colSums(is.na(cov_df))
# Ethnoracial_Group has 280 NA values
# Age has 8

# DJ
```
```{r throw nn 5}
#using the Janitor Package for snake casing each var
cov_df <- cov_df %>%
  clean_names(case = "snake")

# NT
```

```{r throw nn 6}
cov_df <- cov_df %>%
  mutate(class_family = (class_mom + class_dad)/2)

# Let us round up class_family variable to make it whole and also remove class_dad and class_mom variables
cov_df <- cov_df %>%
  mutate_at(vars(class_family), ~ ceiling(.)) %>%
  dplyr::select(-class_mom) %>%
  dplyr::select(-class_dad)

# DJ
```

```{r throw nn 7}
cov_df <- cov_df[, -which(names(cov_df) =="covid_too_much_time")]

dim(cov_df)

# Now we are left with 26 predictors.

# DJ
```
```{r throw nn 8}
# Convert character variables and integers to factors w/ levels
cov_df <- cov_df %>% 
  mutate_if(is.character, as.factor)

cov_df <- cov_df %>% 
  mutate_if(is.integer, as.factor)

set.seed(123) # set.seed(123) for reproducibility
partition <- createDataPartition(cov_df$total_anxiety_level, p = 0.80, list = FALSE)
train_df <- cov_df[partition,]
test_df <- cov_df[-partition,]

dim(train_df)
# 2029 observations

dim(test_df)
# 505

# DJ
```
```{r throw nn 9}
#Using K-nearest-neighbors to fill in the missing values
train_df <- knnImputation(train_df, k = 3)
test_df <- knnImputation(test_df, k = 3)

#checking to see if there are any more N/a values
colSums(is.na(train_df))
colSums(is.na(test_df))
unique(train_df$age)
unique(test_df$age)
unique(train_df$ethnoracial_group)
unique(test_df$ethnoracial_group)

# NT
```

```{r throw nn 10}
# Box-Cox transformation from MASS package
# First, we need to ensure our response variable is positive before transforming.
range(cov_df$total_anxiety_level)
# 0 to 500 is our range.
# Therefore, we need to shift data to ensure it is all positive. We will shift by 1.
cov_df$total_anxiety_level_shift <- cov_df$total_anxiety_level - min(cov_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = cov_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# Optimal lambda is 1.1

# Box-Cox transformation function
boxcox_transform <- function(x, lambda) {
  if (lambda == 0) {
    return(log(x))  # special case for when lambda = 0
  } else {
    return((x^lambda - 1) / lambda)
  }
}

# Now we apply transformation to total_anxiety_level_shift
cov_df$box_cox_anxiety_level <- boxcox_transform(cov_df$total_anxiety_level_shift, lambda_opt)

# histogram
hist(cov_df$box_cox_anxiety_level,
     xlab = "Box-Cox Total Anxiety Level",
     main = "Box-Cox Total Anxiety Level Distribution")

qqnorm(cov_df$box_cox_anxiety_level, main = "Q-Q Plot of Box-Cox Total Anxiety Level")
qqline(cov_df$box_cox_anxiety_level, col = "red")


# Shapiro-Wilk Test
print(shapiro.test(cov_df$box_cox_anxiety_level))

# DJ
```

```{r throw nn 11}
orig_shapiro = shapiro.test(cov_df$total_anxiety_level)
bc_shapiro = shapiro.test(cov_df$box_cox_anxiety_level)

orig_p_val = orig_shapiro$p.value
bc_p_val = bc_shapiro$p.value

print(paste("Original p-value:", orig_p_val))
print(paste("Box-Cox transformed p-value:", bc_p_val))

if (bc_p_val > orig_p_val) {
  print("Yes, p-value after Box-Cox transformation is greater than original p-value")
} else {
  print("No, p-value after Box-Cox transformation is not greater than original p-value")
}

# Now, let us Box-Cox transform our response variable in our training and test data
# Training
range(train_df$total_anxiety_level)
# 0 to 500
train_df$total_anxiety_level_shift <- train_df$total_anxiety_level - min(train_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = train_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# 1.1 again
train_df$box_cox_anxiety_level <- boxcox_transform(train_df$total_anxiety_level_shift, lambda_opt)

# Now, let us remove total_anxiety_level and total_anxiety_level_shift from train_df then rename the box_cox_anxiety_level to total_anxiety_level. Let us also round to the nearest whole number
train_df <- train_df %>%
  dplyr::select(-total_anxiety_level, -total_anxiety_level_shift) %>%
  dplyr::rename(total_anxiety_level = box_cox_anxiety_level) %>%
  dplyr::mutate(total_anxiety_level = round(total_anxiety_level))

# Test Data
range(test_df$total_anxiety_level)
# 0 to 500
test_df$total_anxiety_level_shift <- test_df$total_anxiety_level - min(test_df$total_anxiety_level) + 1
# Now we will fit linear model only using the intercept and no predictors
model <- lm(total_anxiety_level_shift ~ 1, data = test_df)
# use boxcox function from MASS
bc <- boxcox(model, lambda = seq(-5, 5, by = 0.1))
# We want to find optial lambda for transformation
lambda_opt <- bc$x[which.max(bc$y)]
print(lambda_opt)
# 1.1 again
test_df$box_cox_anxiety_level <- boxcox_transform(test_df$total_anxiety_level_shift, lambda_opt)

# Now, let us remove total_anxiety_level and total_anxiety_level_shift from train_df then rename the box_cox_anxiety_level to total_anxiety_level. Let us also round to the nearest whole number
test_df <- test_df %>%
  dplyr::select(-total_anxiety_level, -total_anxiety_level_shift) %>%
  dplyr::rename(total_anxiety_level = box_cox_anxiety_level) %>%
  dplyr::mutate(total_anxiety_level = round(total_anxiety_level))

# DJ
```

One-hot-encoding our categorical variables and scaling/centering predictors.
```{r throw nn 12}
# Creating dummy variables (one-hot-encoding) for the specified columns in train_df

# train_df
onehot_encoder <- dummyVars(~ age + source + ethnoracial_group,
                            train_df[, c("age", "source",
                                         "ethnoracial_group")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
# Predicting dummy variables using the onehot_encoder for train_df
onehot_enc_train <- predict(onehot_encoder,
                            train_df[,c("age", "source",
                                        "ethnoracial_group")])
# Combining the original train_df with the one-hot encoded variables
train_df <- cbind(train_df, onehot_enc_train)

# Snake case train_df
#using the Janitor Package for snake casing each var
train_df <- train_df %>%
  clean_names(case = "snake")

# Remove three variables that were one-hot encoded:
# age, source, ethnoracial_group
feats_to_remove <- c("age", "source", "ethnoracial_group")
train_df <- train_df %>%
  dplyr::select(-feats_to_remove)

# Storing the response variable into a vector (total_anxiety_level)
train_resp <- train_df$total_anxiety_level

# Creating a train_df without response variable
feats_to_remove <- c("total_anxiety_level")
train_df <- train_df %>%
  dplyr::select(-feats_to_remove)

# Before centering and scaling. Check to see if there are any columns that add up to zero in train_df
colSums(train_df)
column_sums <- colSums(train_df)
# Print the column sums
print(column_sums)
# Identify columns with a sum of zero
zero_sum_columns <- names(column_sums[column_sums == 0])
print(zero_sum_columns)
# Returns zero. That means there is no zero sum columns

# Centering and scaling using standard deviation of 2
train_feat <- scale(train_df, center = TRUE, scale = TRUE)
apply(train_feat, 2, sd)


# test_df
# Creating dummy variables (one-hot-encoding) for the specified columns in test_df
onehot_encoder <- dummyVars(~ age + source + ethnoracial_group,
                            test_df[, c("age", "source",
                                         "ethnoracial_group")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)
# Predicting dummy variables using the onehot_encoder for test_df
onehot_enc_test <- predict(onehot_encoder,
                            test_df[,c("age", "source",
                                        "ethnoracial_group")])
# Combining the original test_df with the one-hot encoded variables
test_df <- cbind(test_df, onehot_enc_test)

# Snake case train_df
# Using the Janitor Package for snake casing each var
test_df <- test_df %>%
  clean_names(case = "snake")

# Remove three variables that were one-hot encoded:
# age, source, ethnoracial_group
feats_to_remove <- c("age", "source", "ethnoracial_group")
test_df <- test_df %>%
  dplyr::select(-feats_to_remove)

# Storing the response variable into a vector (total_anxiety_level)
test_resp <- test_df$total_anxiety_level

# Creating a test_df without response variable
feats_to_remove <- c("total_anxiety_level")
test_df <- test_df %>%
  dplyr::select(-feats_to_remove)

# Before centering and scaling. Check to see if there are any columns that add up to zero in test_df
colSums(test_df)
column_sums <- colSums(test_df)
# Print the column sums
print(column_sums)
# Identify columns with a sum of zero
zero_sum_columns <- names(column_sums[column_sums == 0])
print(zero_sum_columns)
# Returns "x65_to_74"
# In this case, we need to remove "x65_to_74" from both train_feat and test_feat

# Remove from train_feat
train_feat <- as.data.frame(train_feat)
feats_to_remove <- c("x65_to_74")
train_feat <- train_feat %>%
  dplyr::select(-feats_to_remove)
# Convert train_feat back to matrix for neural network model
train_feat <- as.matrix(train_feat)

# Scale/center test_feat
test_feat <- scale(test_df, center = TRUE, scale = TRUE)
apply(test_feat, 2, sd)

# Remove "x65_to_74"
test_feat <- as.data.frame(test_feat)
feats_to_remove <- c("x65_to_74")
test_feat <- test_feat %>%
  dplyr::select(-feats_to_remove)
# Convert train_feat back to matrix for neural network model
test_feat <- as.matrix(test_feat)

# NT/DJ
```

```{r throw 14}
# age variable code by NT
#training data
train_df <- train_df %>%
  group_by(age) %>%
  mutate(age_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-age)

#test data
test_df <- test_df %>%
  group_by(age) %>%
  mutate(age_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-age)

# source variable frequency encoding by DJ
# training data
train_df <- train_df %>%
  group_by(source) %>%
  mutate(source_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-source)

# test data
test_df <- test_df %>%
  group_by(source) %>%
  mutate(source_frequency = n()) %>%
  ungroup() %>%
  dplyr::select(-source)


# ethnoracial_group variable one-hot encoding
# training data
dummy_vars = dummyVars(~ ethnoracial_group, 
                       data = train_df,
                       levelsOnly = TRUE,
                       fullRank = FALSE)
encoded_ethno_racial_df <- predict(dummy_vars, newdata = train_df)
# convert col names to lower case
encoded_ethno_racial_df <- encoded_ethno_racial_df %>%
  clean_names(case = "snake")
train_df <- cbind(train_df, encoded_ethno_racial_df)
# Remove ethno_racial variable
train_df <- train_df %>%
  dplyr::select(-ethnoracial_group)

# test data
dummy_vars = dummyVars(~ ethnoracial_group, 
                       data = test_df,
                       levelsOnly = TRUE,
                       fullRank = FALSE)
encoded_ethno_racial_df <- predict(dummy_vars, newdata = test_df)
# convert col names to lower case
encoded_ethno_racial_df <- encoded_ethno_racial_df %>%
  clean_names(case = "snake")
test_df <- cbind(test_df, encoded_ethno_racial_df)
# Remove ethno_racial variable
test_df <- test_df %>%
  dplyr::select(-ethnoracial_group)

# Now, for visual purposes in our data sets, let us move our response variable to the far right of all of our predictors.
train_df <- train_df %>%
  dplyr::select(-total_anxiety_level, total_anxiety_level)
# row names changed from 1 to x1 and so on. Let's reset that.
rownames(train_df) <- NULL

test_df <- test_df %>%
  dplyr::select(-total_anxiety_level, total_anxiety_level)
# row names changed from 1 to x1 and so on. Let's reset that.
rownames(test_df) <- NULL


# DJ/NT
```

```{r throw 15}
set.seed(123)
cov_lm = lm(train_resp ~. - clemson - bmi - educ_dad - univ_montana - educ_mom - hrs_outdoor - nh_black - nh_white - oregon_state - x55_to_64 - limit_groups - penn_state - class_family - univ_utah - limit_exercise_gym - hrs_screen - limit_exercise_home - limit_outdoor_active - educ_self - infected_any - nh_asian - limit_travel - nc_state - class_self - hrs_exercise - x45_to_54 - park_use, train_df)
summary(cov_lm)

pred_train = predict(cov_lm, newdata = train_df)
mean((pred_train - train_df$train_resp)^2)

pred_test = predict(cov_lm, newdata = test_df)
mean((pred_test - test_df$test_resp)^2)

# DJ
```